{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNiV4UrU6Du6PL6Na3T6fd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Patrick-ChenKehan/BE-521-Project/blob/main/truetest_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm for BE 5210 Competition -- MONITR"
      ],
      "metadata": {
        "id": "C1SzsvwdwIH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Upload the submitted `Algorithm_MONITR.zip` and the hidden test set `truetest_data.mat` to colab `/content/`.**\n",
        "\n",
        "After uploading the files, directly running the notebook will generate the required `predictions.mat`."
      ],
      "metadata": {
        "id": "AyvBZ_w8rw4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the Algorithm Files \n",
        "\n",
        "\n",
        "\n",
        "*   `models/` includes all saved model necessary for prediction\n",
        "*   `utils/utils.py` includes all utility functions and feature constructing functions\n",
        "*    `utils/NN_Model.py` includes the dataset class and the MLP class\n",
        "\n"
      ],
      "metadata": {
        "id": "yCux1EwLwwdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Algorithm_MONITR.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b6-5ZFQwwu2",
        "outputId": "3852e452-61fc-46a8-c767-1ab86e30c6a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Algorithm_MONITR.zip\n",
            "   creating: models/\n",
            "  inflating: models/train_std_S1.npy  \n",
            "  inflating: models/train_std_S2.npy  \n",
            "  inflating: models/train_std_S3.npy  \n",
            "  inflating: models/NN_S3.pth        \n",
            "  inflating: models/lgbr_f1_S3.txt   \n",
            "  inflating: models/idx_S1.npy       \n",
            "  inflating: models/XGB_S1.json      \n",
            "  inflating: models/lgbr_f1_S2.txt   \n",
            "  inflating: models/NN_S2.pth        \n",
            "  inflating: models/idx_S2.npy       \n",
            "  inflating: models/idx_S3.npy       \n",
            "  inflating: models/lgbr_f1_S1.txt   \n",
            "  inflating: models/NN_S1.pth        \n",
            "  inflating: models/lgbr_f3_S1.txt   \n",
            "  inflating: models/lgbr_f3_S3.txt   \n",
            "  inflating: models/lgbr_f3_S2.txt   \n",
            "  inflating: models/XGB_S3.json      \n",
            "  inflating: models/XGB_S2.json      \n",
            "  inflating: models/lgbr_f0_S1.txt   \n",
            "  inflating: models/lgbr_f0_S2.txt   \n",
            "  inflating: models/lgbr_f0_S3.txt   \n",
            "  inflating: models/lgbr_f2_S2.txt   \n",
            "  inflating: models/train_mean_S2.npy  \n",
            "  inflating: models/train_mean_S3.npy  \n",
            "  inflating: models/lgbr_f2_S3.txt   \n",
            "  inflating: models/train_mean_S1.npy  \n",
            "  inflating: models/lgbr_f2_S1.txt   \n",
            "   creating: utils/\n",
            "  inflating: utils/.DS_Store         \n",
            "  inflating: __MACOSX/utils/._.DS_Store  \n",
            "  inflating: utils/__init__.py       \n",
            "   creating: utils/__pycache__/\n",
            "  inflating: utils/utils.py          \n",
            "  inflating: utils/NN_model.py       \n",
            "  inflating: utils/__pycache__/NN_model.cpython-311.pyc  \n",
            "  inflating: utils/__pycache__/utils.cpython-311.pyc  \n",
            "  inflating: utils/__pycache__/__init__.cpython-311.pyc  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and install related libraries"
      ],
      "metadata": {
        "id": "ZovAAwiFwjef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up the notebook environment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import scipy\n",
        "from scipy.stats import pearsonr\n",
        "from scipy import signal as sig\n",
        "import xgboost as xgb\n",
        "from utils import *\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm\n",
        "import torch"
      ],
      "metadata": {
        "id": "uTf3wcQowqGH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load data\n",
        "\n",
        "Upload the hidden test set `truetest_data.mat` to `/content/`."
      ],
      "metadata": {
        "id": "_pWwenIdwcRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw = scipy.io.loadmat('./truetest_data.mat')\n",
        "ecog_1 = raw['truetest_data'][0][0]\n",
        "ecog_2 = raw['truetest_data'][1][0]\n",
        "ecog_3 = raw['truetest_data'][2][0]"
      ],
      "metadata": {
        "id": "m3waZGPSwbwf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute features and R_Matrix"
      ],
      "metadata": {
        "id": "O0aKgRBJpFiM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXdsMhCYwHX6",
        "outputId": "88fe0a49-8ee5-4305-8951-993d9cf3406f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 62, using nperseg = 62\n",
            "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
            "/usr/local/lib/python3.9/dist-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 48, using nperseg = 48\n",
            "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
            "/usr/local/lib/python3.9/dist-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 64, using nperseg = 64\n",
            "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
          ]
        }
      ],
      "source": [
        "# Set up params for windows\n",
        "winLen = 100 / 1e3\n",
        "winOverlap = 50 / 1e3\n",
        "winDisp = winLen - winOverlap\n",
        "\n",
        "# Compute features\n",
        "feature_1 = get_windowed_feats(ecog_1, 1000, winLen, winOverlap)\n",
        "feature_2 = get_windowed_feats(ecog_2, 1000, winLen, winOverlap)\n",
        "feature_3 = get_windowed_feats(ecog_3, 1000, winLen, winOverlap)\n",
        "\n",
        "# Load feature indices\n",
        "idx_1 = np.load('./models/idx_S1.npy')\n",
        "idx_2 = np.load('./models/idx_S2.npy')\n",
        "idx_3 = np.load('./models/idx_S3.npy')\n",
        "\n",
        "# Compute R_matrix of 20 windows and select features with indices\n",
        "R_1 = create_R_matrix(feature_1, 20)[:, idx_1]\n",
        "R_2 = create_R_matrix(feature_2, 20)[:, idx_2]\n",
        "R_3 = create_R_matrix(feature_3, 20)[:, idx_3]\n",
        "R_list = [R_1, R_2, R_3]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and Prediction"
      ],
      "metadata": {
        "id": "MOs69f4cx_BN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load mean and std of the train data for normalization\n",
        "\n",
        "train_mean_S1 = np.load('./models/train_mean_S1.npy')\n",
        "train_std_S1 = np.load('./models/train_std_S1.npy')\n",
        "\n",
        "train_mean_S2 = np.load('./models/train_mean_S2.npy')\n",
        "train_std_S2 = np.load('./models/train_std_S2.npy')\n",
        "\n",
        "train_mean_S3 = np.load('./models/train_mean_S3.npy')\n",
        "train_std_S3 = np.load('./models/train_std_S3.npy')\n",
        "\n",
        "train_mean_ls = [train_mean_S1, train_mean_S2, train_mean_S3]\n",
        "train_std_ls = [train_std_S1, train_std_S2, train_std_S3]\n",
        "\n",
        "device = torch.device(\"cpu\") # Using CPU for convenience"
      ],
      "metadata": {
        "id": "-z6CN86Cx2rw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "for i in range(3): # for each subject\n",
        "    \n",
        "    # Load LGBM for each finger and predict\n",
        "    lgbm_reg_list = [lightgbm.Booster(model_file=f'./models/lgbr_f{j}_S{i + 1}.txt') for j in range(4)]\n",
        "    \n",
        "    prediction_lgbm_list = [lgbm_reg.predict(R_list[i]) for lgbm_reg in lgbm_reg_list]\n",
        "    prediction_lgbm = np.vstack(prediction_lgbm_list).T\n",
        "    \n",
        "    # Load normalized data into the dataloader\n",
        "    leaderboard_dataset = FingerFeatureDataset((R_list[i] - train_mean_ls[i]) / train_std_ls[i], np.zeros(R_list[i].shape[0]).copy())\n",
        "    dataloader = DataLoader(leaderboard_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    # Load MLP and predict\n",
        "    net = FingerRegressor(R_list[i].shape[1], 4).to(device)\n",
        "    net.load_state_dict(torch.load(f'./models/NN_S{i + 1}.pth', map_location=device))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pred = []\n",
        "        net.eval()\n",
        "        for i, (ecog, dg) in enumerate(dataloader):\n",
        "            ecog = ecog.to(device)\n",
        "            dg = dg.to(device)\n",
        "            output = net(ecog).to(device)\n",
        "            pred += [output.detach().cpu().numpy()]\n",
        "\n",
        "    prediction_NN = np.concatenate(pred)\n",
        "    \n",
        "    # Ensemble the NN prediction and LightGBM prediction by averaging\n",
        "    prediction = (prediction_lgbm + prediction_NN) / 2\n",
        "    predictions.append(prediction)"
      ],
      "metadata": {
        "id": "Pvc5kFYgyFnc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pack prediction to file for submission"
      ],
      "metadata": {
        "id": "Zov78mhVqdxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pack_submission(predictions, \"predictions.mat\", \"predicted_dg\")"
      ],
      "metadata": {
        "id": "KQ2f_0HKztaB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check submission file\n",
        "raw = scipy.io.loadmat('./predictions.mat')\n",
        "raw['predicted_dg'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9bAmmS6z7a3",
        "outputId": "de14bc97-f54f-4946-d2ff-cd0542585d74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1, 147500, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}