{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.signal as sig\n",
    "from scipy.stats import pearsonr\n",
    "from utils import *\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(0)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = scipy.io.loadmat('./datasets/raw_training_data.mat')\n",
    "data_glove_1 = raw['train_dg'][0][0]\n",
    "data_glove_1 = np.delete(data_glove_1, 3, 1)\n",
    "data_glove_2 = raw['train_dg'][1][0]\n",
    "data_glove_2 = np.delete(data_glove_2, 3, 1)\n",
    "data_glove_3 = raw['train_dg'][2][0]\n",
    "data_glove_3 = np.delete(data_glove_3, 3, 1)\n",
    "\n",
    "ecog_1 = raw['train_ecog'][0][0]\n",
    "ecog_2 = raw['train_ecog'][1][0]\n",
    "ecog_3 = raw['train_ecog'][2][0]\n",
    "\n",
    "labels_1 = np.argmax(data_glove_1, axis=1)\n",
    "labels_2 = np.argmax(data_glove_2, axis=1)\n",
    "labels_3 = np.argmax(data_glove_3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_1 = filter_data(ecog_1)\n",
    "ecog_2 = filter_data(ecog_2)\n",
    "ecog_3 = filter_data(ecog_3)\n",
    "\n",
    "train_test_ratio = 0.7\n",
    "\n",
    "ecog_1_train = ecog_1[:int(train_test_ratio * ecog_1.shape[0])]\n",
    "ecog_1_test = ecog_1[int(train_test_ratio * ecog_1.shape[0]):]\n",
    "data_glove_1_train = data_glove_1[:int(train_test_ratio * data_glove_1.shape[0])]\n",
    "data_glove_1_test = data_glove_1[int(train_test_ratio * data_glove_1.shape[0]):]\n",
    "\n",
    "ecog_2_train = ecog_2[:int(train_test_ratio * ecog_2.shape[0])]\n",
    "ecog_2_test = ecog_2[int(train_test_ratio * ecog_2.shape[0]):]\n",
    "data_glove_2_train = data_glove_2[:int(train_test_ratio * data_glove_2.shape[0])]\n",
    "data_glove_2_test = data_glove_2[int(train_test_ratio * data_glove_2.shape[0]):]\n",
    "\n",
    "ecog_3_train = ecog_3[:int(train_test_ratio * ecog_3.shape[0])]\n",
    "ecog_3_test = ecog_3[int(train_test_ratio * ecog_3.shape[0]):]\n",
    "data_glove_3_train = data_glove_3[:int(train_test_ratio * data_glove_3.shape[0])]\n",
    "data_glove_3_test = data_glove_3[int(train_test_ratio * data_glove_3.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NumWins(x, fs, winLen, winDisp):\n",
    "    return int(1 + (x.shape[0] - winLen * fs) / (winDisp * fs))\n",
    "\n",
    "winLen = 200 / 1e3\n",
    "winOverlap = 40 / 1e3\n",
    "winDisp = winLen - winOverlap\n",
    "NumWins(ecog_1, 1000, winLen, winDisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineLength(x):\n",
    "    return np.abs(np.diff(x, axis=0)).sum(axis=0)\n",
    "\n",
    "def Area(x):\n",
    "    return np.abs(x).sum(axis=0)\n",
    "\n",
    "def Energy(x):\n",
    "    return (x ** 2).sum(axis=0)\n",
    "\n",
    "def ZeroCrossingMean(x):\n",
    "    return ((x < x.mean(axis=0))[1:] & (x[:-1] > x.mean(axis=0)) | (x > x.mean(axis=0))[1:] & (x[:-1] < x.mean(axis=0))).sum(axis=0)\n",
    "\n",
    "def numSpikes(x):\n",
    "    #TODO: implement\n",
    "    sig.find_peaks(x, height=0, distance=100)\n",
    "    pass\n",
    "\n",
    "def averageTimeDomain(x):\n",
    "    #TODO: implement\n",
    "    return np.mean(x, axis=0)\n",
    "\n",
    "def bandpower(x, fs, fmin, fmax):\n",
    "    fs = 1000\n",
    "    # win = 4 * sf\n",
    "    freqs, psd = sig.welch(x, fs, axis=0, nperseg=x.shape[0])\n",
    "    \n",
    "    # Define delta lower and upper limits\n",
    "    # fmin, fmax = 0.5, 4\n",
    "\n",
    "    # Find intersecting values in frequency vector\n",
    "    idx_delta = np.logical_and(freqs >= fmin, freqs <= fmax)\n",
    "    \n",
    "    from scipy.integrate import simps\n",
    "\n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]  # = 1 / 4 = 0.25\n",
    "\n",
    "    # Compute the absolute power by approximating the area under the curve\n",
    "    delta_power = simps(psd[idx_delta], dx=freq_res, axis=0)\n",
    "    \n",
    "    return delta_power\n",
    "\n",
    "def spectral_entropy(x, fs=1000):\n",
    "    # Calculate the power spectrum\n",
    "    f, Pxx = sig.welch(x, fs=fs)\n",
    "    # Normalize the power spectrum\n",
    "    Pxx_norm = Pxx / Pxx.sum()\n",
    "    # Calculate the spectral entropy\n",
    "    se = -1 * (Pxx_norm * np.log2(Pxx_norm)).sum()\n",
    "    return se\n",
    "\n",
    "def hjorth_complexity(x):\n",
    "    dx = np.diff(x)\n",
    "    d2x = np.diff(dx)\n",
    "    var_x = np.var(x)\n",
    "    var_dx = np.var(dx)\n",
    "    var_d2x = np.var(d2x)\n",
    "    activity = var_x\n",
    "    mobility = np.sqrt(var_d2x / var_dx)\n",
    "    # Calculate Hjorth complexity\n",
    "    complexity = mobility / activity\n",
    "    return complexity\n",
    "    \n",
    "# Kurtosis = @(x) ((1/size(x,1))*sum((x - mean(x)).^4))./(((1/size(x,1))*sum((x - mean(x)).^2)).^2);\n",
    "def Kurtosis(x):\n",
    "    return ((1/x.shape[0])*np.sum((x - np.mean(x))**4))/(((1/x.shape[0])*np.sum((x - np.mean(x))**2))**2)\n",
    "\n",
    "def get_features(filtered_window, fs=1000):\n",
    "    \"\"\"\n",
    "        Write a function that calculates features for a given filtered window. \n",
    "        Feel free to use features you have seen before in this class, features that\n",
    "        have been used in the literature, or design your own!\n",
    "\n",
    "        Input: \n",
    "        filtered_window (window_samples x channels): the window of the filtered ecog signal \n",
    "        fs: sampling rate\n",
    "        Output:\n",
    "        features (channels x num_features): the features calculated on each channel for the window\n",
    "    \"\"\"\n",
    "    feat_LL = LineLength(filtered_window)\n",
    "    feat_Area = Area(filtered_window)\n",
    "    feat_Energy = Energy(filtered_window)\n",
    "    feat_ZCM = ZeroCrossingMean(filtered_window)\n",
    "    feat_TimeAvg = averageTimeDomain(filtered_window)\n",
    "    feat_SpectralEntropy = spectral_entropy(filtered_window)\n",
    "    feat_Hijorth = hjorth_complexity(filtered_window)\n",
    "    feat_kurtosis = Kurtosis(filtered_window)\n",
    "    # feat_FreqAvg = averageFreqDomain(filtered_window)\n",
    "    \n",
    "    from pyriemann.estimation import Covariances\n",
    "    from pyriemann.tangentspace import TangentSpace\n",
    "    \n",
    "    # covar = Covariances().fit_transform(np.expand_dims(filtered_window.T, 0))\n",
    "    # # covest = Covariances('oas')\n",
    "    # # temp = np.expand_dims(filtered_window, axis=-1)\n",
    "    # # covar = covest.fit_transform(temp)\n",
    "    # ts = TangentSpace()\n",
    "    # tsfeat = ts.fit_transform(covar)\n",
    "    # # print(tsfeat.shape)\n",
    "\n",
    "    # raise notImplementedError()\n",
    "    return np.hstack([#feat_LL, feat_Area, feat_Energy, feat_ZCM, \n",
    "                      feat_TimeAvg, \n",
    "                      #feat_SpectralEntropy,\n",
    "                      feat_Hijorth,\n",
    "                      feat_kurtosis,\n",
    "                      bandpower(filtered_window, 1000, 5, 15),\n",
    "                      bandpower(filtered_window, 1000, 20, 25),\n",
    "                      bandpower(filtered_window, 1000, 75, 115),\n",
    "                      bandpower(filtered_window, 1000, 125, 160),\n",
    "                      bandpower(filtered_window, 1000, 160, 175)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_feats(raw_ecog, fs, window_length, window_overlap):\n",
    "    \"\"\"\n",
    "        Write a function which processes data through the steps of filtering and\n",
    "        feature calculation and returns features. Points will be awarded for completing\n",
    "        each step appropriately (note that if one of the functions you call within this script\n",
    "        returns a bad output, you won't be double penalized). Note that you will need\n",
    "        to run the filter_data and get_features functions within this function. \n",
    "\n",
    "        Inputs:\n",
    "        raw_eeg (samples x channels): the raw signal\n",
    "        fs: the sampling rate (1000 for this dataset)\n",
    "        window_length: the window's length\n",
    "        window_overlap: the window's overlap\n",
    "        Output: \n",
    "        all_feats (num_windows x (channels x features)): the features for each channel for each time window\n",
    "            note that this is a 2D array. \n",
    "    \"\"\"\n",
    "    raw_ecog = filter_data(raw_ecog, fs)\n",
    "    \n",
    "    window_disp = window_length - window_overlap\n",
    "    \n",
    "    all_feats = np.vstack([get_features(raw_ecog[int(i * window_disp * fs):int(i * window_disp * fs + window_length * fs), :], fs) for i in range(NumWins(raw_ecog, fs, window_length, window_disp\n",
    "))])\n",
    "    \n",
    "    return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5013630.20390603])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ecog_1[:500]\n",
    "convar = np.cov(temp, rowvar=False)\n",
    "convar[0, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_R_matrix(features, N_wind):\n",
    "    \"\"\" \n",
    "    Write a function to calculate the R matrix\n",
    "\n",
    "    Input:\n",
    "        features (samples (number of windows in the signal) x channels x features): \n",
    "        the features you calculated using get_windowed_feats\n",
    "        N_wind: number of windows to use in the R matrix\n",
    "\n",
    "    Output:\n",
    "        R (samples x (N_wind*channels*features))\n",
    "    \"\"\"\n",
    "    num_win = features.shape[0]\n",
    "    num_channel_features = features.shape[1]\n",
    "    \n",
    "    # Append a copy of the first N-1 rows to the beginning of features\n",
    "    features = np.vstack((features[:N_wind-1], features))\n",
    "    \n",
    "    R = np.zeros((num_win, N_wind * num_channel_features))\n",
    "    \n",
    "    for i in range(num_win):\n",
    "        # Get the feature matrix for the current window\n",
    "        # Resize the feature matrix and store in R\n",
    "        R[i,:] = features[i:i+N_wind,:].reshape(-1)\n",
    "\n",
    "    R = np.hstack((np.ones((R.shape[0], 1)), R))\n",
    "\n",
    "    return R\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "winLen = 200 / 1e3\n",
    "winOverlap = 40 / 1e3\n",
    "winDisp = winLen - winOverlap\n",
    "\n",
    "feature_train = get_windowed_feats(ecog_1_train, 1000, winLen, winOverlap)\n",
    "R_train = create_R_matrix(feature_train, 20)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_1_test, 1000, winLen, winOverlap)\n",
    "R_test = create_R_matrix(feature_test, 20)\n",
    "\n",
    "y_train = data_glove_1_train\n",
    "y_test = data_glove_1_test\n",
    "y_train = sig.resample(y_train, R_train.shape[0], axis=0)\n",
    "y_test = sig.resample(y_test, R_test.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(562, 7481)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerFeatureDataset(Dataset):\n",
    "    def __init__(self, ecog, dg, window=2000):\n",
    "        self.ecog = np.float32(ecog + np.random.normal(0, 0.01, ecog.shape))\n",
    "        self.ecog = (self.ecog - self.ecog.mean(axis=0)) / self.ecog.std(axis=0)\n",
    "#         self.ecog = self.ecog.reshape(self.ecog.shape[0], 1, -1)\n",
    "        self.dg = np.float32(dg)\n",
    "        \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecog)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.ecog[idx], self.dg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s1_train = FingerFeatureDataset(R_train.copy(), y_train.copy())\n",
    "dataset_s1_valid = FingerFeatureDataset(R_test.copy(), y_test.copy())\n",
    "\n",
    "train_loader = DataLoader(dataset_s1_train, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(dataset_s1_valid, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerRegressor(nn.Module):\n",
    "    def __init__(self, num_fingers) -> None:\n",
    "        super(FingerRegressor, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, 61, stride=4)\n",
    "        self.pool1 = nn.AvgPool1d(16, 16)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(7481, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(16, num_fingers)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "#         nn.init.xavier_normal_(self.fc2.weight)\n",
    "#         nn.init.xavier_normal_(self.fc3.weight)\n",
    "        nn.init.xavier_normal_(self.fc4.weight)\n",
    "        self.dropout1 = nn.Dropout(0.8)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "#         x = self.relu(self.conv1(x))\n",
    "#         x = self.pool1(x)\n",
    "        \n",
    "#         x= self.dropout2(x)\n",
    "        \n",
    "#         x = self.flatten(x)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "#         x = self.relu(self.fc3(x))\n",
    "        \n",
    "        output = self.fc4(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss: 2.378 | Train Cor: -0.016 | Valid loss: 0.827 | Valid Cor: 0.001615105653582246\n",
      "Epoch 2 | Train loss: 1.129 | Train Cor: 0.018 | Valid loss: 0.813 | Valid Cor: 0.03079252470395183\n",
      "Epoch 3 | Train loss: 1.038 | Train Cor: 0.050 | Valid loss: 0.810 | Valid Cor: 0.027552631337388188\n",
      "Epoch 4 | Train loss: 1.003 | Train Cor: 0.081 | Valid loss: 0.808 | Valid Cor: 0.05312594004161772\n",
      "Epoch 5 | Train loss: 0.997 | Train Cor: 0.078 | Valid loss: 0.808 | Valid Cor: 0.06655137603871591\n",
      "Epoch 6 | Train loss: 0.988 | Train Cor: 0.112 | Valid loss: 0.804 | Valid Cor: 0.10800499369773034\n",
      "Epoch 7 | Train loss: 0.972 | Train Cor: 0.173 | Valid loss: 0.800 | Valid Cor: 0.11572320596536767\n",
      "Epoch 8 | Train loss: 0.973 | Train Cor: 0.167 | Valid loss: 0.797 | Valid Cor: 0.12635799301394215\n",
      "Epoch 9 | Train loss: 0.961 | Train Cor: 0.199 | Valid loss: 0.795 | Valid Cor: 0.14112172741615106\n",
      "Epoch 10 | Train loss: 0.949 | Train Cor: 0.237 | Valid loss: 0.790 | Valid Cor: 0.15781378460080345\n",
      "Epoch 11 | Train loss: 0.941 | Train Cor: 0.246 | Valid loss: 0.791 | Valid Cor: 0.1277910731559937\n",
      "Epoch 12 | Train loss: 0.942 | Train Cor: 0.244 | Valid loss: 0.794 | Valid Cor: 0.1466163186142183\n",
      "Epoch 13 | Train loss: 0.932 | Train Cor: 0.263 | Valid loss: 0.789 | Valid Cor: 0.14396074605805084\n",
      "Epoch 14 | Train loss: 0.923 | Train Cor: 0.287 | Valid loss: 0.796 | Valid Cor: 0.11864159498087423\n",
      "Epoch 15 | Train loss: 0.905 | Train Cor: 0.324 | Valid loss: 0.792 | Valid Cor: 0.13681959330603735\n",
      "Epoch 16 | Train loss: 0.923 | Train Cor: 0.280 | Valid loss: 0.795 | Valid Cor: 0.12635850407298646\n",
      "Epoch 17 | Train loss: 0.916 | Train Cor: 0.295 | Valid loss: 0.788 | Valid Cor: 0.17624391640365816\n",
      "Epoch 18 | Train loss: 0.909 | Train Cor: 0.311 | Valid loss: 0.782 | Valid Cor: 0.18531877755544318\n",
      "Epoch 19 | Train loss: 0.884 | Train Cor: 0.355 | Valid loss: 0.776 | Valid Cor: 0.19106174326938244\n",
      "Epoch 20 | Train loss: 0.914 | Train Cor: 0.297 | Valid loss: 0.779 | Valid Cor: 0.21073634178353304\n",
      "Epoch 21 | Train loss: 0.876 | Train Cor: 0.368 | Valid loss: 0.788 | Valid Cor: 0.182417142348492\n",
      "Epoch 22 | Train loss: 0.896 | Train Cor: 0.321 | Valid loss: 0.781 | Valid Cor: 0.2049171784201853\n",
      "Epoch 23 | Train loss: 0.890 | Train Cor: 0.347 | Valid loss: 0.776 | Valid Cor: 0.21780141781620221\n",
      "Epoch 24 | Train loss: 0.893 | Train Cor: 0.336 | Valid loss: 0.778 | Valid Cor: 0.21882897487582337\n",
      "Epoch 25 | Train loss: 0.906 | Train Cor: 0.313 | Valid loss: 0.781 | Valid Cor: 0.2006640160291502\n",
      "Epoch 26 | Train loss: 0.895 | Train Cor: 0.337 | Valid loss: 0.778 | Valid Cor: 0.22343614181578994\n",
      "Epoch 27 | Train loss: 0.890 | Train Cor: 0.334 | Valid loss: 0.781 | Valid Cor: 0.20924987487787974\n",
      "Epoch 28 | Train loss: 0.899 | Train Cor: 0.329 | Valid loss: 0.778 | Valid Cor: 0.23681735767461526\n",
      "Epoch 29 | Train loss: 0.893 | Train Cor: 0.339 | Valid loss: 0.777 | Valid Cor: 0.21612408241295372\n",
      "Epoch 30 | Train loss: 0.881 | Train Cor: 0.360 | Valid loss: 0.787 | Valid Cor: 0.206919237647219\n",
      "Epoch 31 | Train loss: 0.894 | Train Cor: 0.332 | Valid loss: 0.785 | Valid Cor: 0.19776682245224703\n",
      "Epoch 32 | Train loss: 0.891 | Train Cor: 0.335 | Valid loss: 0.785 | Valid Cor: 0.21112923395259317\n",
      "Epoch 33 | Train loss: 0.897 | Train Cor: 0.331 | Valid loss: 0.789 | Valid Cor: 0.18859477986637413\n",
      "Epoch 34 | Train loss: 0.898 | Train Cor: 0.329 | Valid loss: 0.781 | Valid Cor: 0.20579088900539066\n",
      "Epoch 35 | Train loss: 0.893 | Train Cor: 0.340 | Valid loss: 0.784 | Valid Cor: 0.1883424403737161\n",
      "Epoch 36 | Train loss: 0.886 | Train Cor: 0.351 | Valid loss: 0.783 | Valid Cor: 0.19459426038503413\n",
      "Epoch 37 | Train loss: 0.908 | Train Cor: 0.308 | Valid loss: 0.780 | Valid Cor: 0.21011144612901692\n",
      "Epoch 38 | Train loss: 0.885 | Train Cor: 0.352 | Valid loss: 0.776 | Valid Cor: 0.21145674945922466\n",
      "Epoch 39 | Train loss: 0.881 | Train Cor: 0.352 | Valid loss: 0.789 | Valid Cor: 0.19013601688004617\n",
      "Epoch 40 | Train loss: 0.877 | Train Cor: 0.359 | Valid loss: 0.786 | Valid Cor: 0.20707988341869826\n",
      "Epoch 41 | Train loss: 0.871 | Train Cor: 0.368 | Valid loss: 0.784 | Valid Cor: 0.20050240390542234\n",
      "Epoch 42 | Train loss: 0.880 | Train Cor: 0.351 | Valid loss: 0.788 | Valid Cor: 0.17274084718860633\n",
      "Epoch 43 | Train loss: 0.897 | Train Cor: 0.330 | Valid loss: 0.782 | Valid Cor: 0.21736155334319726\n",
      "Epoch 44 | Train loss: 0.892 | Train Cor: 0.340 | Valid loss: 0.771 | Valid Cor: 0.24556868805058257\n",
      "Epoch 45 | Train loss: 0.896 | Train Cor: 0.335 | Valid loss: 0.773 | Valid Cor: 0.23353953325475124\n",
      "Epoch 46 | Train loss: 0.878 | Train Cor: 0.360 | Valid loss: 0.777 | Valid Cor: 0.24865699033345295\n",
      "Epoch 47 | Train loss: 0.892 | Train Cor: 0.333 | Valid loss: 0.783 | Valid Cor: 0.20721660071419318\n",
      "Epoch 48 | Train loss: 0.878 | Train Cor: 0.357 | Valid loss: 0.778 | Valid Cor: 0.205549825983335\n",
      "Epoch 49 | Train loss: 0.881 | Train Cor: 0.353 | Valid loss: 0.780 | Valid Cor: 0.21931555862646612\n",
      "Epoch 50 | Train loss: 0.884 | Train Cor: 0.355 | Valid loss: 0.767 | Valid Cor: 0.2525937042151527\n"
     ]
    }
   ],
   "source": [
    "# net = EEGNet().to(device)\n",
    "# net = EEGNetRegressor(4).to(device)\n",
    "# net = EEGNet(n_classes=4, channels=62, samples=3000).to(device) #.cuda(0)\n",
    "net = FingerRegressor(4).to(device)\n",
    "#print (net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)))))#.cuda(0))))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for (i, (ecog, dg)) in enumerate(train_loader):\n",
    "        # print(ecog.shape)\n",
    "        ecog = ecog.to(device)\n",
    "        dg = dg.to(device)\n",
    "        output = net(ecog)\n",
    "        pred += [output.detach().cpu().numpy()]\n",
    "        loss = criterion(output, dg)\n",
    "        # print (loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    pred = np.concatenate(pred)\n",
    "    train_cor = correlation_dl(sig.resample(pred, len(data_glove_1_train)), data_glove_1_train)[1]\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    # train_acc = correct / total\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred = []\n",
    "        for (i, (ecog, dg)) in enumerate(test_loader):\n",
    "            ecog = ecog.to(device)\n",
    "            dg = dg.to(device)\n",
    "            output = net(ecog).to(device)\n",
    "            pred += [output.detach().cpu().numpy()]\n",
    "            loss = criterion(output, dg)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        pred = np.concatenate(pred)\n",
    "        val_cor = correlation_dl(sig.resample(pred, len(data_glove_1_test)), data_glove_1_test)[1]\n",
    "        \n",
    "        valid_loss = running_loss / len(test_loader)\n",
    "    # print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Acc: {train_acc:.3f} | Valid loss: {valid_loss:.3f} | Valid Acc: {val_cor}')\n",
    "    print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Cor: {train_cor:.3f} | Valid loss: {valid_loss:.3f} | Valid Cor: {val_cor}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.3327978217891593,\n",
       "  0.39574460065139055,\n",
       "  0.019683200175373804,\n",
       "  0.2680101301775504],\n",
       " 0.25405893819836856)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
