{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.signal as sig\n",
    "from scipy.stats import pearsonr\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(0)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = scipy.io.loadmat('./datasets/raw_training_data.mat')\n",
    "data_glove_1 = raw['train_dg'][0][0]\n",
    "data_glove_1_train = np.delete(data_glove_1, 3, 1)\n",
    "data_glove_2 = raw['train_dg'][1][0]\n",
    "data_glove_2_train = np.delete(data_glove_2, 3, 1)\n",
    "data_glove_3 = raw['train_dg'][2][0]\n",
    "data_glove_3_train = np.delete(data_glove_3, 3, 1)\n",
    "\n",
    "ecog_1_train = raw['train_ecog'][0][0]\n",
    "ecog_2_train = raw['train_ecog'][1][0]\n",
    "ecog_3_train = raw['train_ecog'][2][0]\n",
    "\n",
    "\n",
    "raw = scipy.io.loadmat('./datasets/sub1_comp.mat')\n",
    "ecog_1_comp = raw['train_data']\n",
    "dg_1_comp = raw['train_dg']\n",
    "ecog_1_valid = raw['test_data'][49000:]\n",
    "\n",
    "raw = scipy.io.loadmat('./datasets/sub2_comp.mat')\n",
    "ecog_2_comp = raw['train_data']\n",
    "dg_2_comp = raw['train_dg']\n",
    "ecog_2_valid = raw['test_data'][49000:]\n",
    "\n",
    "raw = scipy.io.loadmat('./datasets/sub3_comp.mat')\n",
    "ecog_3_comp = raw['train_data']\n",
    "dg_3_comp = raw['train_dg']\n",
    "ecog_3_valid = raw['test_data'][49000:]\n",
    "\n",
    "dg_1_raw = scipy.io.loadmat('./datasets/sub1_testlabels.mat')\n",
    "dg_1_valid = dg_1_raw['test_dg'][49000:]\n",
    "dg_1_valid = np.delete(dg_1_valid, 3, 1)\n",
    "\n",
    "dg_2_raw = scipy.io.loadmat('./datasets/sub2_testlabels.mat')\n",
    "dg_2_valid = dg_2_raw['test_dg'][49000:]\n",
    "dg_2_valid = np.delete(dg_2_valid, 3, 1)\n",
    "\n",
    "dg_3_raw = scipy.io.loadmat('./datasets/sub3_testlabels.mat')\n",
    "dg_3_valid = dg_3_raw['test_dg'][49000:]\n",
    "dg_3_valid = np.delete(dg_3_valid, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "winLen = 100 / 1e3\n",
    "winOverlap = 50 / 1e3\n",
    "winDisp = winLen - winOverlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerFeatureDataset(Dataset):\n",
    "    def __init__(self, R, dg, window=2000):\n",
    "        self.R = np.float32(R)\n",
    "#         self.R = (self.R - ecog_1_train.mean(axis=0)) / ecog_1_train.std(axis=0)\n",
    "#         self.ecog = self.ecog.reshape(self.ecog.shape[0], 1, -1)\n",
    "        self.dg = np.float32(dg)\n",
    "        \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.R)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.R[idx], self.dg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 62, using nperseg = 62\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: invalid value encountered in divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: invalid value encountered in divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: invalid value encountered in divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(615,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: invalid value encountered in divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n"
     ]
    }
   ],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_1_train, 1000, winLen, winOverlap)\n",
    "# R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_1_valid, 1000, winLen, winOverlap)\n",
    "# R_test = create_R_matrix(feature_test, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_1_train\n",
    "Y_test = dg_1_valid\n",
    "Y_train = sig.resample(Y_train, feature_train.shape[0], axis=0)\n",
    "Y_test = sig.resample(Y_test, feature_test.shape[0], axis=0)\n",
    "\n",
    "R_train = create_R_matrix(feature_train, 20)\n",
    "R_test = create_R_matrix(feature_test, 20)\n",
    "\n",
    "idx_1 = feature_selection(R_train, Y_train, 800)\n",
    "print(idx_1.shape)\n",
    "\n",
    "R_train = R_train[:, idx_1]\n",
    "train_mean_S1 = R_train.mean(axis=0)\n",
    "train_std_S1 = R_train.std(axis=0)\n",
    "R_train = (R_train - train_mean_S1) / train_std_S1\n",
    "R_test = R_test[:, idx_1]\n",
    "R_test = (R_test - train_mean_S1) / train_std_S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s1_train = FingerFeatureDataset(R_train.copy(), Y_train.copy())\n",
    "dataset_s1_valid = FingerFeatureDataset(R_test.copy(), Y_test.copy())\n",
    "\n",
    "train_loader = DataLoader(dataset_s1_train, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(dataset_s1_valid, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerRegressor(nn.Module):\n",
    "    def __init__(self, num_features, num_fingers) -> None:\n",
    "        super(FingerRegressor, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_features, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, num_fingers)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "        nn.init.xavier_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout2 = nn.Dropout(0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        output = self.fc3(x)\n",
    "\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss: 1.504 | Train Cor: 0.101 | Valid loss: 0.964 | Valid Cor: 0.2791198923690979\n",
      "Epoch 2 | Train loss: 0.947 | Train Cor: 0.252 | Valid loss: 0.909 | Valid Cor: 0.3544631229099592\n",
      "Epoch 3 | Train loss: 0.855 | Train Cor: 0.333 | Valid loss: 0.836 | Valid Cor: 0.42061805638053645\n",
      "Epoch 4 | Train loss: 0.794 | Train Cor: 0.398 | Valid loss: 0.819 | Valid Cor: 0.4342122278402871\n",
      "Epoch 5 | Train loss: 0.756 | Train Cor: 0.434 | Valid loss: 0.790 | Valid Cor: 0.4519443780075736\n",
      "Epoch 6 | Train loss: 0.726 | Train Cor: 0.460 | Valid loss: 0.767 | Valid Cor: 0.4635174213254888\n",
      "Epoch 7 | Train loss: 0.696 | Train Cor: 0.484 | Valid loss: 0.751 | Valid Cor: 0.4722486487915055\n",
      "Epoch 8 | Train loss: 0.693 | Train Cor: 0.487 | Valid loss: 0.757 | Valid Cor: 0.4746569787244396\n",
      "Epoch 9 | Train loss: 0.670 | Train Cor: 0.507 | Valid loss: 0.731 | Valid Cor: 0.4875335511530971\n",
      "Epoch 10 | Train loss: 0.658 | Train Cor: 0.519 | Valid loss: 0.728 | Valid Cor: 0.49992471960217727\n",
      "Epoch 11 | Train loss: 0.637 | Train Cor: 0.537 | Valid loss: 0.722 | Valid Cor: 0.5015308942958588\n",
      "Epoch 12 | Train loss: 0.622 | Train Cor: 0.549 | Valid loss: 0.727 | Valid Cor: 0.4986627687453413\n",
      "Epoch 13 | Train loss: 0.625 | Train Cor: 0.546 | Valid loss: 0.727 | Valid Cor: 0.49984504943077607\n",
      "Epoch 14 | Train loss: 0.615 | Train Cor: 0.557 | Valid loss: 0.720 | Valid Cor: 0.5049383702988829\n",
      "Epoch 15 | Train loss: 0.607 | Train Cor: 0.568 | Valid loss: 0.718 | Valid Cor: 0.5093033938031388\n",
      "Epoch 16 | Train loss: 0.617 | Train Cor: 0.562 | Valid loss: 0.713 | Valid Cor: 0.5183890513798555\n",
      "Epoch 17 | Train loss: 0.589 | Train Cor: 0.585 | Valid loss: 0.716 | Valid Cor: 0.5258006872681141\n",
      "Epoch 18 | Train loss: 0.600 | Train Cor: 0.581 | Valid loss: 0.728 | Valid Cor: 0.5264184682733591\n",
      "Epoch 19 | Train loss: 0.586 | Train Cor: 0.594 | Valid loss: 0.727 | Valid Cor: 0.5292910370361035\n",
      "Epoch 20 | Train loss: 0.574 | Train Cor: 0.605 | Valid loss: 0.715 | Valid Cor: 0.5347300093657478\n",
      "Epoch 21 | Train loss: 0.567 | Train Cor: 0.610 | Valid loss: 0.723 | Valid Cor: 0.5335251560468649\n",
      "Epoch 22 | Train loss: 0.555 | Train Cor: 0.625 | Valid loss: 0.714 | Valid Cor: 0.5370933745863098\n",
      "Epoch 23 | Train loss: 0.565 | Train Cor: 0.617 | Valid loss: 0.714 | Valid Cor: 0.5466666645116511\n",
      "Epoch 24 | Train loss: 0.556 | Train Cor: 0.627 | Valid loss: 0.709 | Valid Cor: 0.5429076353022161\n",
      "Epoch 25 | Train loss: 0.531 | Train Cor: 0.649 | Valid loss: 0.711 | Valid Cor: 0.5449935983473971\n",
      "Epoch 26 | Train loss: 0.553 | Train Cor: 0.633 | Valid loss: 0.706 | Valid Cor: 0.5511184063577846\n",
      "Epoch 27 | Train loss: 0.534 | Train Cor: 0.648 | Valid loss: 0.707 | Valid Cor: 0.5498658054561544\n",
      "Epoch 28 | Train loss: 0.525 | Train Cor: 0.657 | Valid loss: 0.715 | Valid Cor: 0.5399423924700709\n",
      "Epoch 29 | Train loss: 0.527 | Train Cor: 0.655 | Valid loss: 0.716 | Valid Cor: 0.5482481266885358\n",
      "Epoch 30 | Train loss: 0.523 | Train Cor: 0.657 | Valid loss: 0.708 | Valid Cor: 0.5502480869027273\n"
     ]
    }
   ],
   "source": [
    "net = FingerRegressor(R_train.shape[1], 4).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for (i, (ecog, dg)) in enumerate(train_loader):\n",
    "        # print(ecog.shape)\n",
    "        ecog = ecog.to(device)\n",
    "        dg = dg.to(device)\n",
    "        output = net(ecog)\n",
    "        pred += [output.detach().cpu().numpy()]\n",
    "        loss = criterion(output, dg)\n",
    "        # print (loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    pred = np.concatenate(pred)\n",
    "    train_cor = correlation_dl(sig.resample(pred, len(data_glove_1_train)), data_glove_1_train)[1]\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    # train_acc = correct / total\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred = []\n",
    "        for (i, (ecog, dg)) in enumerate(test_loader):\n",
    "            ecog = ecog.to(device)\n",
    "            dg = dg.to(device)\n",
    "            output = net(ecog).to(device)\n",
    "            pred += [output.detach().cpu().numpy()]\n",
    "            loss = criterion(output, dg)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        pred = np.concatenate(pred)\n",
    "        val_cor = correlation_dl(sig.resample(pred, len(dg_1_valid)), dg_1_valid)[1]\n",
    "        \n",
    "        valid_loss = running_loss / len(test_loader)\n",
    "    # print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Acc: {train_acc:.3f} | Valid loss: {valid_loss:.3f} | Valid Acc: {val_cor}')\n",
    "    print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Cor: {train_cor:.3f} | Valid loss: {valid_loss:.3f} | Valid Cor: {val_cor}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./models/train_mean_S1', train_mean_S1)\n",
    "np.save('./models/train_std_S1', train_std_S1)\n",
    "torch.save(net.state_dict(), './models/NN_S1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 48, using nperseg = 48\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m feature_train \u001b[38;5;241m=\u001b[39m get_windowed_feats(ecog_2_train, \u001b[38;5;241m1000\u001b[39m, winLen, winOverlap)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# R_train = create_R_matrix(feature_train, 5)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m feature_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_windowed_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mecog_2_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwinLen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwinOverlap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# R_test = create_R_matrix(feature_test, 5)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Downsample the glove data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m data_glove_2_train\n",
      "File \u001b[0;32m~/BE-521-Project/utils/utils.py:202\u001b[0m, in \u001b[0;36mget_windowed_feats\u001b[0;34m(raw_ecog, fs, window_length, window_overlap)\u001b[0m\n\u001b[1;32m    198\u001b[0m raw_ecog \u001b[38;5;241m=\u001b[39m filter_data(raw_ecog, fs)\n\u001b[1;32m    200\u001b[0m window_disp \u001b[38;5;241m=\u001b[39m window_length \u001b[38;5;241m-\u001b[39m window_overlap\n\u001b[0;32m--> 202\u001b[0m all_feats \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([get_features(raw_ecog[\u001b[38;5;28mint\u001b[39m(i \u001b[38;5;241m*\u001b[39m window_disp \u001b[38;5;241m*\u001b[39m fs):\u001b[38;5;28mint\u001b[39m(i \u001b[38;5;241m*\u001b[39m window_disp \u001b[38;5;241m*\u001b[39m fs \u001b[38;5;241m+\u001b[39m window_length \u001b[38;5;241m*\u001b[39m fs), :], fs) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NumWins(raw_ecog, fs, window_length, window_disp))])\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_feats\n",
      "File \u001b[0;32m~/BE-521-Project/utils/utils.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m raw_ecog \u001b[38;5;241m=\u001b[39m filter_data(raw_ecog, fs)\n\u001b[1;32m    200\u001b[0m window_disp \u001b[38;5;241m=\u001b[39m window_length \u001b[38;5;241m-\u001b[39m window_overlap\n\u001b[0;32m--> 202\u001b[0m all_feats \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_ecog\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_disp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_disp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NumWins(raw_ecog, fs, window_length, window_disp))])\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_feats\n",
      "File \u001b[0;32m~/BE-521-Project/utils/utils.py:179\u001b[0m, in \u001b[0;36mget_features\u001b[0;34m(filtered_window, fs)\u001b[0m\n\u001b[1;32m    162\u001b[0m feat_kurtosis \u001b[38;5;241m=\u001b[39m Kurtosis(filtered_window)\n\u001b[1;32m    163\u001b[0m feat_covariance \u001b[38;5;241m=\u001b[39m Covariance(filtered_window)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mhstack([feat_LL, \n\u001b[1;32m    166\u001b[0m                   feat_Area, \n\u001b[1;32m    167\u001b[0m                 \u001b[38;5;66;03m#   feat_covariance,\u001b[39;00m\n\u001b[1;32m    168\u001b[0m                   feat_Energy, \n\u001b[1;32m    169\u001b[0m                   feat_ZCM, \n\u001b[1;32m    170\u001b[0m                   feat_TimeAvg, \n\u001b[1;32m    171\u001b[0m                   feat_SpectralEntropy,\n\u001b[1;32m    172\u001b[0m                   feat_Hijorth,\n\u001b[1;32m    173\u001b[0m                   feat_kurtosis,\n\u001b[1;32m    174\u001b[0m                   feat_covariance,\n\u001b[1;32m    175\u001b[0m                   bandpower(filtered_window, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m15\u001b[39m),\n\u001b[1;32m    176\u001b[0m                   bandpower(filtered_window, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m25\u001b[39m),\n\u001b[1;32m    177\u001b[0m                   bandpower(filtered_window, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m115\u001b[39m),\n\u001b[1;32m    178\u001b[0m                   bandpower(filtered_window, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m125\u001b[39m, \u001b[38;5;241m160\u001b[39m),\n\u001b[0;32m--> 179\u001b[0m                   \u001b[43mbandpower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m175\u001b[39;49m\u001b[43m)\u001b[49m])\n",
      "File \u001b[0;32m~/BE-521-Project/utils/utils.py:83\u001b[0m, in \u001b[0;36mbandpower\u001b[0;34m(x, fs, fmin, fmax)\u001b[0m\n\u001b[1;32m     81\u001b[0m fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# win = 4 * sf\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m freqs, psd \u001b[38;5;241m=\u001b[39m \u001b[43msig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwelch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Define delta lower and upper limits\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# fmin, fmax = 0.5, 4\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Find intersecting values in frequency vector\u001b[39;00m\n\u001b[1;32m     89\u001b[0m idx_delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_and(freqs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m fmin, freqs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m fmax)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:455\u001b[0m, in \u001b[0;36mwelch\u001b[0;34m(x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, average)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwelch\u001b[39m(x, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhann\u001b[39m\u001b[38;5;124m'\u001b[39m, nperseg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, noverlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nfft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    298\u001b[0m           detrend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, return_onesided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensity\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    299\u001b[0m           axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    Estimate power spectral density using Welch's method.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     freqs, Pxx \u001b[38;5;241m=\u001b[39m \u001b[43mcsd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnperseg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoverlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mreturn_onesided\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_onesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                     \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m freqs, Pxx\u001b[38;5;241m.\u001b[39mreal\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:590\u001b[0m, in \u001b[0;36mcsd\u001b[0;34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, average)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcsd\u001b[39m(x, y, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhann\u001b[39m\u001b[38;5;124m'\u001b[39m, nperseg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, noverlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nfft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    464\u001b[0m         detrend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, return_onesided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdensity\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    465\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    466\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m    Estimate the cross power spectral density, Pxy, using Welch's method.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m \n\u001b[1;32m    589\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m     freqs, _, Pxy \u001b[38;5;241m=\u001b[39m \u001b[43m_spectral_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnfft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_onesided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpsd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# Average over windows.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(Pxy\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m Pxy\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:1901\u001b[0m, in \u001b[0;36m_spectral_helper\u001b[0;34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode, boundary, padded)\u001b[0m\n\u001b[1;32m   1898\u001b[0m         result[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1899\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m         \u001b[38;5;66;03m# Last point is unpaired Nyquist freq point, don't double\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m         result[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1903\u001b[0m time \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(nperseg\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m nperseg\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1904\u001b[0m                  nperseg \u001b[38;5;241m-\u001b[39m noverlap)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mfloat\u001b[39m(fs)\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m boundary \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_2_train, 1000, winLen, winOverlap)\n",
    "# R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_2_valid, 1000, winLen, winOverlap)\n",
    "# R_test = create_R_matrix(feature_test, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_2_train\n",
    "Y_test = dg_2_valid\n",
    "Y_train = sig.resample(Y_train, feature_train.shape[0], axis=0)\n",
    "Y_test = sig.resample(Y_test, feature_test.shape[0], axis=0)\n",
    "\n",
    "R_train = create_R_matrix(feature_train, 20)\n",
    "R_test = create_R_matrix(feature_test, 20)\n",
    "\n",
    "idx_2 = feature_selection(R_train, Y_train, 800)\n",
    "print(idx_2.shape)\n",
    "\n",
    "R_train = R_train[:, idx_2]\n",
    "train_mean_S2 = R_train.mean(axis=0)\n",
    "train_std_S2 = R_train.std(axis=0)\n",
    "R_train = (R_train - train_mean_S2) / train_std_S2\n",
    "R_test = R_test[:, idx_2]\n",
    "R_test = (R_test - train_mean_S2) / train_std_S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s2_train = FingerFeatureDataset(R_train.copy(), Y_train.copy())\n",
    "dataset_s2_valid = FingerFeatureDataset(R_test.copy(), Y_test.copy())\n",
    "\n",
    "train_loader = DataLoader(dataset_s2_train, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(dataset_s2_valid, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss: 1.180 | Train Cor: 0.123 | Valid loss: 0.903 | Valid Cor: 0.22764925068312003\n",
      "Epoch 2 | Train loss: 0.876 | Train Cor: 0.248 | Valid loss: 0.863 | Valid Cor: 0.2948895611416057\n",
      "Epoch 3 | Train loss: 0.835 | Train Cor: 0.283 | Valid loss: 0.831 | Valid Cor: 0.3015851873945617\n",
      "Epoch 4 | Train loss: 0.789 | Train Cor: 0.344 | Valid loss: 0.814 | Valid Cor: 0.34566783621195535\n",
      "Epoch 5 | Train loss: 0.754 | Train Cor: 0.391 | Valid loss: 0.785 | Valid Cor: 0.3848998740871513\n",
      "Epoch 6 | Train loss: 0.724 | Train Cor: 0.441 | Valid loss: 0.790 | Valid Cor: 0.3967953174992602\n",
      "Epoch 7 | Train loss: 0.703 | Train Cor: 0.468 | Valid loss: 0.767 | Valid Cor: 0.4168834035142266\n",
      "Epoch 8 | Train loss: 0.694 | Train Cor: 0.479 | Valid loss: 0.768 | Valid Cor: 0.422192753143098\n",
      "Epoch 9 | Train loss: 0.673 | Train Cor: 0.505 | Valid loss: 0.741 | Valid Cor: 0.4234267026077596\n",
      "Epoch 10 | Train loss: 0.664 | Train Cor: 0.517 | Valid loss: 0.762 | Valid Cor: 0.42633104085426077\n",
      "Epoch 11 | Train loss: 0.650 | Train Cor: 0.530 | Valid loss: 0.752 | Valid Cor: 0.43392958126423253\n",
      "Epoch 12 | Train loss: 0.643 | Train Cor: 0.537 | Valid loss: 0.754 | Valid Cor: 0.4268738954349852\n",
      "Epoch 13 | Train loss: 0.630 | Train Cor: 0.547 | Valid loss: 0.764 | Valid Cor: 0.423644656652702\n",
      "Epoch 14 | Train loss: 0.628 | Train Cor: 0.550 | Valid loss: 0.758 | Valid Cor: 0.42248888690677333\n",
      "Epoch 15 | Train loss: 0.619 | Train Cor: 0.559 | Valid loss: 0.740 | Valid Cor: 0.42591169241721305\n",
      "Epoch 16 | Train loss: 0.625 | Train Cor: 0.554 | Valid loss: 0.754 | Valid Cor: 0.4364535681725127\n",
      "Epoch 17 | Train loss: 0.620 | Train Cor: 0.556 | Valid loss: 0.747 | Valid Cor: 0.4296034185697107\n",
      "Epoch 18 | Train loss: 0.604 | Train Cor: 0.572 | Valid loss: 0.746 | Valid Cor: 0.4379086458446092\n",
      "Epoch 19 | Train loss: 0.585 | Train Cor: 0.588 | Valid loss: 0.729 | Valid Cor: 0.4422702060777939\n",
      "Epoch 20 | Train loss: 0.592 | Train Cor: 0.586 | Valid loss: 0.756 | Valid Cor: 0.44357022150349523\n",
      "Epoch 21 | Train loss: 0.579 | Train Cor: 0.597 | Valid loss: 0.746 | Valid Cor: 0.45691359291303846\n",
      "Epoch 22 | Train loss: 0.568 | Train Cor: 0.608 | Valid loss: 0.738 | Valid Cor: 0.4539907795879876\n",
      "Epoch 23 | Train loss: 0.574 | Train Cor: 0.600 | Valid loss: 0.743 | Valid Cor: 0.476921312316\n",
      "Epoch 24 | Train loss: 0.555 | Train Cor: 0.614 | Valid loss: 0.730 | Valid Cor: 0.46936724640112126\n",
      "Epoch 25 | Train loss: 0.546 | Train Cor: 0.627 | Valid loss: 0.719 | Valid Cor: 0.4844448967190263\n",
      "Epoch 26 | Train loss: 0.545 | Train Cor: 0.625 | Valid loss: 0.712 | Valid Cor: 0.4878829905173671\n",
      "Epoch 27 | Train loss: 0.537 | Train Cor: 0.633 | Valid loss: 0.707 | Valid Cor: 0.48961837389479845\n",
      "Epoch 28 | Train loss: 0.538 | Train Cor: 0.631 | Valid loss: 0.705 | Valid Cor: 0.5019398999451435\n",
      "Epoch 29 | Train loss: 0.526 | Train Cor: 0.642 | Valid loss: 0.708 | Valid Cor: 0.5045317226883342\n",
      "Epoch 30 | Train loss: 0.533 | Train Cor: 0.638 | Valid loss: 0.695 | Valid Cor: 0.5027277126971572\n"
     ]
    }
   ],
   "source": [
    "net = FingerRegressor(R_train.shape[1], 4).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for (i, (ecog, dg)) in enumerate(train_loader):\n",
    "        # print(ecog.shape)\n",
    "        ecog = ecog.to(device)\n",
    "        dg = dg.to(device)\n",
    "        output = net(ecog)\n",
    "        pred += [output.detach().cpu().numpy()]\n",
    "        loss = criterion(output, dg)\n",
    "        # print (loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    pred = np.concatenate(pred)\n",
    "    train_cor = correlation_dl(sig.resample(pred, len(data_glove_2_train)), data_glove_2_train)[1]\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    # train_acc = correct / total\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred = []\n",
    "        for (i, (ecog, dg)) in enumerate(test_loader):\n",
    "            ecog = ecog.to(device)\n",
    "            dg = dg.to(device)\n",
    "            output = net(ecog).to(device)\n",
    "            pred += [output.detach().cpu().numpy()]\n",
    "            loss = criterion(output, dg)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        pred = np.concatenate(pred)\n",
    "        val_cor = correlation_dl(sig.resample(pred, len(dg_2_valid)), dg_2_valid)[1]\n",
    "        \n",
    "        valid_loss = running_loss / len(test_loader)\n",
    "    # print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Acc: {train_acc:.3f} | Valid loss: {valid_loss:.3f} | Valid Acc: {val_cor}')\n",
    "    print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Cor: {train_cor:.3f} | Valid loss: {valid_loss:.3f} | Valid Cor: {val_cor}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./models/train_mean_S2', train_mean_S2)\n",
    "np.save('./models/train_std_S2', train_std_S2)\n",
    "torch.save(net.state_dict(), './models/NN_S2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 64, using nperseg = 64\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: invalid value encountered in divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: invalid value encountered in divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: invalid value encountered in divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: divide by zero encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:289: RuntimeWarning: invalid value encountered in divide\n",
      "  correlation_coefficient /= X_norms\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: invalid value encountered in divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n"
     ]
    }
   ],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_3_train, 1000, winLen, winOverlap)\n",
    "# R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_3_valid, 1000, winLen, winOverlap)\n",
    "# R_test = create_R_matrix(feature_test, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_3_train\n",
    "Y_test = dg_3_valid\n",
    "Y_train = sig.resample(Y_train, feature_train.shape[0], axis=0)\n",
    "Y_test = sig.resample(Y_test, feature_test.shape[0], axis=0)\n",
    "\n",
    "R_train = create_R_matrix(feature_train, 20)\n",
    "R_test = create_R_matrix(feature_test, 20)\n",
    "\n",
    "idx_3 = feature_selection(R_train, Y_train, 800)\n",
    "print(idx_3.shape)\n",
    "\n",
    "R_train = R_train[:, idx_3]\n",
    "train_mean_S3 = R_train.mean(axis=0)\n",
    "train_std_S3 = R_train.std(axis=0)\n",
    "R_train = (R_train - train_mean_S3) / train_std_S3\n",
    "R_test = R_test[:, idx_3]\n",
    "R_test = (R_test - train_mean_S3) / train_std_S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s3_train = FingerFeatureDataset(R_train.copy(), Y_train.copy())\n",
    "dataset_s3_valid = FingerFeatureDataset(R_test.copy(), Y_test.copy())\n",
    "\n",
    "train_loader = DataLoader(dataset_s3_train, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(dataset_s3_valid, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss: 0.972 | Train Cor: 0.217 | Valid loss: 0.702 | Valid Cor: 0.5027024159343293\n",
      "Epoch 2 | Train loss: 0.726 | Train Cor: 0.456 | Valid loss: 0.581 | Valid Cor: 0.6261185812030904\n",
      "Epoch 3 | Train loss: 0.601 | Train Cor: 0.573 | Valid loss: 0.522 | Valid Cor: 0.6732182618683221\n",
      "Epoch 4 | Train loss: 0.547 | Train Cor: 0.622 | Valid loss: 0.496 | Valid Cor: 0.6950261718740653\n",
      "Epoch 5 | Train loss: 0.520 | Train Cor: 0.644 | Valid loss: 0.490 | Valid Cor: 0.7044172981382085\n",
      "Epoch 6 | Train loss: 0.494 | Train Cor: 0.668 | Valid loss: 0.482 | Valid Cor: 0.7091621003933387\n",
      "Epoch 7 | Train loss: 0.476 | Train Cor: 0.684 | Valid loss: 0.474 | Valid Cor: 0.7155927608430317\n",
      "Epoch 8 | Train loss: 0.466 | Train Cor: 0.690 | Valid loss: 0.463 | Valid Cor: 0.7178863615545731\n",
      "Epoch 9 | Train loss: 0.466 | Train Cor: 0.691 | Valid loss: 0.471 | Valid Cor: 0.7157061504442087\n",
      "Epoch 10 | Train loss: 0.450 | Train Cor: 0.703 | Valid loss: 0.468 | Valid Cor: 0.7182267598521104\n",
      "Epoch 11 | Train loss: 0.440 | Train Cor: 0.709 | Valid loss: 0.467 | Valid Cor: 0.718655900449706\n",
      "Epoch 12 | Train loss: 0.427 | Train Cor: 0.722 | Valid loss: 0.468 | Valid Cor: 0.7200675792923553\n",
      "Epoch 13 | Train loss: 0.430 | Train Cor: 0.718 | Valid loss: 0.464 | Valid Cor: 0.7190884360669341\n",
      "Epoch 14 | Train loss: 0.422 | Train Cor: 0.724 | Valid loss: 0.465 | Valid Cor: 0.7184200246463914\n",
      "Epoch 15 | Train loss: 0.421 | Train Cor: 0.724 | Valid loss: 0.466 | Valid Cor: 0.7158411886326377\n",
      "Epoch 16 | Train loss: 0.417 | Train Cor: 0.727 | Valid loss: 0.467 | Valid Cor: 0.7197465840296317\n",
      "Epoch 17 | Train loss: 0.426 | Train Cor: 0.722 | Valid loss: 0.470 | Valid Cor: 0.7204547905098857\n",
      "Epoch 18 | Train loss: 0.409 | Train Cor: 0.734 | Valid loss: 0.461 | Valid Cor: 0.7231932976462141\n",
      "Epoch 19 | Train loss: 0.419 | Train Cor: 0.727 | Valid loss: 0.461 | Valid Cor: 0.7189975856257218\n",
      "Epoch 20 | Train loss: 0.419 | Train Cor: 0.727 | Valid loss: 0.457 | Valid Cor: 0.721361005284088\n",
      "Epoch 21 | Train loss: 0.403 | Train Cor: 0.739 | Valid loss: 0.468 | Valid Cor: 0.7214946696373018\n",
      "Epoch 22 | Train loss: 0.408 | Train Cor: 0.734 | Valid loss: 0.466 | Valid Cor: 0.7187433165107938\n",
      "Epoch 23 | Train loss: 0.405 | Train Cor: 0.739 | Valid loss: 0.462 | Valid Cor: 0.7190106026023293\n",
      "Epoch 24 | Train loss: 0.406 | Train Cor: 0.736 | Valid loss: 0.464 | Valid Cor: 0.7165799820435675\n",
      "Epoch 25 | Train loss: 0.400 | Train Cor: 0.739 | Valid loss: 0.462 | Valid Cor: 0.7217082370941847\n",
      "Epoch 26 | Train loss: 0.406 | Train Cor: 0.737 | Valid loss: 0.463 | Valid Cor: 0.7201574900435619\n",
      "Epoch 27 | Train loss: 0.402 | Train Cor: 0.739 | Valid loss: 0.463 | Valid Cor: 0.7216539765150688\n",
      "Epoch 28 | Train loss: 0.402 | Train Cor: 0.740 | Valid loss: 0.451 | Valid Cor: 0.7251618355824399\n",
      "Epoch 29 | Train loss: 0.395 | Train Cor: 0.746 | Valid loss: 0.458 | Valid Cor: 0.7231963345027634\n",
      "Epoch 30 | Train loss: 0.393 | Train Cor: 0.744 | Valid loss: 0.461 | Valid Cor: 0.7206420967609866\n"
     ]
    }
   ],
   "source": [
    "# net = EEGNet().to(device)\n",
    "# net = EEGNetRegressor(4).to(device)\n",
    "# net = EEGNet(n_classes=4, channels=62, samples=3000).to(device) #.cuda(0)\n",
    "net = FingerRegressor(R_train.shape[1], 4).to(device)\n",
    "#print (net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)))))#.cuda(0))))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for (i, (ecog, dg)) in enumerate(train_loader):\n",
    "        # print(ecog.shape)\n",
    "        ecog = ecog.to(device)\n",
    "        dg = dg.to(device)\n",
    "        output = net(ecog)\n",
    "        pred += [output.detach().cpu().numpy()]\n",
    "        loss = criterion(output, dg)\n",
    "        # print (loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    pred = np.concatenate(pred)\n",
    "    train_cor = correlation_dl(sig.resample(pred, len(data_glove_3_train)), data_glove_3_train)[1]\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    # train_acc = correct / total\n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred = []\n",
    "        for (i, (ecog, dg)) in enumerate(test_loader):\n",
    "            ecog = ecog.to(device)\n",
    "            dg = dg.to(device)\n",
    "            output = net(ecog).to(device)\n",
    "            pred += [output.detach().cpu().numpy()]\n",
    "            loss = criterion(output, dg)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        pred = np.concatenate(pred)\n",
    "        val_cor = correlation_dl(sig.resample(pred, len(dg_3_valid)), dg_3_valid)[1]\n",
    "        \n",
    "        valid_loss = running_loss / len(test_loader)\n",
    "    # print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Acc: {train_acc:.3f} | Valid loss: {valid_loss:.3f} | Valid Acc: {val_cor}')\n",
    "    print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Cor: {train_cor:.3f} | Valid loss: {valid_loss:.3f} | Valid Cor: {val_cor}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./models/train_mean_S3', train_mean_S3)\n",
    "np.save('./models/train_std_S3', train_std_S3)\n",
    "torch.save(net.state_dict(), './models/NN_S3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 62, using nperseg = 62\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 48, using nperseg = 48\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 64, using nperseg = 64\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    }
   ],
   "source": [
    "ecog_1_leaderboard = ecog_1_comp[500: 500 + 147500]\n",
    "dg_1_leaderboard = dg_1_comp[500: 500 + 147500]\n",
    "\n",
    "ecog_2_leaderboard = ecog_2_comp[500: 500 + 147500]\n",
    "dg_2_leaderboard = dg_2_comp[500: 500 + 147500]\n",
    "\n",
    "ecog_3_leaderboard = ecog_3_comp[500: 500 + 147500]\n",
    "dg_3_leaderboard = dg_3_comp[500: 500 + 147500]\n",
    "\n",
    "winLen = 100 / 1e3\n",
    "winOverlap = 50 / 1e3\n",
    "winDisp = winLen - winOverlap\n",
    "\n",
    "\n",
    "feature_1 = get_windowed_feats(ecog_1_leaderboard, 1000, winLen, winOverlap)\n",
    "# R_1 = create_R_matrix(feature_1, 5)\n",
    "feature_2 = get_windowed_feats(ecog_2_leaderboard, 1000, winLen, winOverlap)\n",
    "# R_2 = create_R_matrix(feature_2, 5)\n",
    "feature_3 = get_windowed_feats(ecog_3_leaderboard, 1000, winLen, winOverlap)\n",
    "# R_3 = create_R_matrix(feature_3, 5)\n",
    "\n",
    "# idx_1 = np.load('./models/idx_S1.npy')\n",
    "# idx_2 = np.load('./models/idx_S2.npy')\n",
    "# idx_3 = np.load('./models/idx_S3.npy')\n",
    "\n",
    "R_1 = create_R_matrix(feature_1, 20)[:, idx_1]\n",
    "R_2 = create_R_matrix(feature_2, 20)[:, idx_2]\n",
    "R_3 = create_R_matrix(feature_3, 20)[:, idx_3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_S1 = np.load('./models/train_mean_S1.npy')\n",
    "train_std_S1 = np.load('./models/train_std_S1.npy')\n",
    "\n",
    "train_mean_S2 = np.load('./models/train_mean_S2.npy')\n",
    "train_std_S2 = np.load('./models/train_std_S2.npy')\n",
    "\n",
    "train_mean_S3 = np.load('./models/train_mean_S3.npy')\n",
    "train_std_S3 = np.load('./models/train_std_S3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5553528686861728"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard_dataset = FingerFeatureDataset((R_1 - train_mean_S1) / train_std_S1, np.zeros(R_1.shape[0]).copy())\n",
    "dataloader = DataLoader(leaderboard_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "net = FingerRegressor(R_1.shape[1], 4).to(device)\n",
    "net.load_state_dict(torch.load('./models/NN_S1.pth'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = []\n",
    "    net.eval()\n",
    "    for i, (ecog, dg) in enumerate(dataloader):\n",
    "        ecog = ecog.to(device)\n",
    "        dg = dg.to(device)\n",
    "        output = net(ecog).to(device)\n",
    "        pred += [output.detach().cpu().numpy()]\n",
    "\n",
    "    pred = np.concatenate(pred)\n",
    "correlation_dl(sig.resample(pred, len(dg_1_leaderboard)), dg_1_leaderboard)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5654337122490332"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard_dataset = FingerFeatureDataset((R_2 - train_mean_S2) / train_std_S2, np.zeros(R_2.shape[0]).copy())\n",
    "dataloader = DataLoader(leaderboard_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "net = FingerRegressor(R_2.shape[1], 4).to(device)\n",
    "net.load_state_dict(torch.load('./models/NN_S2.pth'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = []\n",
    "    net.eval()\n",
    "    for i, (ecog, dg) in enumerate(dataloader):\n",
    "        ecog = ecog.to(device)\n",
    "        dg = dg.to(device)\n",
    "        output = net(ecog).to(device)\n",
    "        pred += [output.detach().cpu().numpy()]\n",
    "\n",
    "    pred = np.concatenate(pred)\n",
    "correlation_dl(sig.resample(pred, len(dg_2_leaderboard)), dg_2_leaderboard)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6924273174017038"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard_dataset = FingerFeatureDataset((R_3 - train_mean_S3) / train_std_S3, np.zeros(R_3.shape[0]).copy())\n",
    "dataloader = DataLoader(leaderboard_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "net = FingerRegressor(R_3.shape[1], 4).to(device)\n",
    "net.load_state_dict(torch.load('./models/NN_S3.pth'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = []\n",
    "    net.eval()\n",
    "    for i, (ecog, dg) in enumerate(dataloader):\n",
    "        ecog = ecog.to(device)\n",
    "        dg = dg.to(device)\n",
    "        output = net(ecog).to(device)\n",
    "        pred += [output.detach().cpu().numpy()]\n",
    "\n",
    "    pred = np.concatenate(pred)\n",
    "correlation_dl(sig.resample(pred, len(dg_3_leaderboard)), dg_3_leaderboard)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
