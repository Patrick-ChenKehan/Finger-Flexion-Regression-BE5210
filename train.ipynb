{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Model Saving Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the notebook environment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import signal as sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = scipy.io.loadmat('./datasets/raw_training_data.mat')\n",
    "data_glove_1 = raw['train_dg'][0][0]\n",
    "data_glove_1 = np.delete(data_glove_1, 3, 1)\n",
    "data_glove_2 = raw['train_dg'][1][0]\n",
    "data_glove_2 = np.delete(data_glove_2, 3, 1)\n",
    "data_glove_3 = raw['train_dg'][2][0]\n",
    "data_glove_3 = np.delete(data_glove_3, 3, 1)\n",
    "\n",
    "ecog_1 = raw['train_ecog'][0][0]\n",
    "ecog_2 = raw['train_ecog'][1][0]\n",
    "ecog_3 = raw['train_ecog'][2][0]\n",
    "\n",
    "labels_1 = np.argmax(data_glove_1, axis=1)\n",
    "labels_2 = np.argmax(data_glove_2, axis=1)\n",
    "labels_3 = np.argmax(data_glove_3, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(raw_eeg, fs=1000):\n",
    "    \"\"\"\n",
    "    Write a filter function to clean underlying data.\n",
    "    Filter type and parameters are up to you. Points will be awarded for reasonable filter type, parameters and application.\n",
    "    Please note there are many acceptable answers, but make sure you aren't throwing out crucial data or adversly\n",
    "    distorting the underlying data!\n",
    "\n",
    "    Input: \n",
    "        raw_eeg (samples x channels): the raw signal\n",
    "        fs: the sampling rate (1000 for this dataset)\n",
    "    Output: \n",
    "        clean_data (samples x channels): the filtered signal\n",
    "    \"\"\"\n",
    "    dim = 100\n",
    "    # b = sig.firwin(numtaps=dim + 1, cutoff=[0.15, 200], pass_zero='bandpass', fs=fs)\n",
    "    b, a = sig.butter(N=2, Wn=[0.15, 200], btype='bandpass', fs=fs, output='ba')\n",
    "    filtered_eeg = sig.filtfilt(b, a, x=raw_eeg, axis=0)\n",
    "    \n",
    "    return filtered_eeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train test split and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before splitting: 300000 samples\n",
      "After splitting: 210000 training samples and 90000 testing samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_ratio = 0.7\n",
    "\n",
    "ecog_1_train = ecog_1[:int(train_test_ratio * ecog_1.shape[0])]\n",
    "ecog_1_test = ecog_1[int(train_test_ratio * ecog_1.shape[0]):]\n",
    "data_glove_1_train = data_glove_1[:int(train_test_ratio * data_glove_1.shape[0])]\n",
    "data_glove_1_test = data_glove_1[int(train_test_ratio * data_glove_1.shape[0]):]\n",
    "\n",
    "ecog_2_train = ecog_2[:int(train_test_ratio * ecog_2.shape[0])]\n",
    "ecog_2_test = ecog_2[int(train_test_ratio * ecog_2.shape[0]):]\n",
    "data_glove_2_train = data_glove_2[:int(train_test_ratio * data_glove_2.shape[0])]\n",
    "data_glove_2_test = data_glove_2[int(train_test_ratio * data_glove_2.shape[0]):]\n",
    "\n",
    "ecog_3_train = ecog_3[:int(train_test_ratio * ecog_3.shape[0])]\n",
    "ecog_3_test = ecog_3[int(train_test_ratio * ecog_3.shape[0]):]\n",
    "data_glove_3_train = data_glove_3[:int(train_test_ratio * data_glove_3.shape[0])]\n",
    "data_glove_3_test = data_glove_3[int(train_test_ratio * data_glove_3.shape[0]):]\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "Before splitting: {ecog_1.shape[0]} samples\n",
    "After splitting: {ecog_1_train.shape[0]} training samples and {ecog_1_test.shape[0]} testing samples\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NumWins(x, fs, winLen, winDisp):\n",
    "    return int(1 + (x.shape[0] - winLen * fs) / (winDisp * fs))\n",
    "\n",
    "winLen = 200 / 1e3\n",
    "winOverlap = 40 / 1e3\n",
    "winDisp = winLen - winOverlap\n",
    "NumWins(ecog_1, 1000, winLen, winDisp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineLength(x):\n",
    "    return np.abs(np.diff(x, axis=0)).sum(axis=0)\n",
    "\n",
    "def Area(x):\n",
    "    return np.abs(x).sum(axis=0)\n",
    "\n",
    "def Energy(x):\n",
    "    return (x ** 2).sum(axis=0)\n",
    "\n",
    "def ZeroCrossingMean(x):\n",
    "    return ((x < x.mean(axis=0))[1:] & (x[:-1] > x.mean(axis=0)) | (x > x.mean(axis=0))[1:] & (x[:-1] < x.mean(axis=0))).sum(axis=0)\n",
    "\n",
    "def numSpikes(x):\n",
    "    #TODO: implement\n",
    "    sig.find_peaks(x, height=0, distance=100)\n",
    "    pass\n",
    "\n",
    "def averageTimeDomain(x):\n",
    "    #TODO: implement\n",
    "    return np.mean(x, axis=0)\n",
    "\n",
    "def bandpower(x, fs, fmin, fmax):\n",
    "    fs = 1000\n",
    "    # win = 4 * sf\n",
    "    freqs, psd = sig.welch(x, fs, axis=0, nperseg=x.shape[0])\n",
    "    \n",
    "    # Define delta lower and upper limits\n",
    "    # fmin, fmax = 0.5, 4\n",
    "\n",
    "    # Find intersecting values in frequency vector\n",
    "    idx_delta = np.logical_and(freqs >= fmin, freqs <= fmax)\n",
    "    \n",
    "    from scipy.integrate import simps\n",
    "\n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]  # = 1 / 4 = 0.25\n",
    "\n",
    "    # Compute the absolute power by approximating the area under the curve\n",
    "    delta_power = simps(psd[idx_delta], dx=freq_res, axis=0)\n",
    "    \n",
    "    return delta_power\n",
    "\n",
    "def spectral_entropy(x, fs=1000):\n",
    "    # Calculate the power spectrum\n",
    "    f, Pxx = sig.welch(x, fs=fs)\n",
    "    # Normalize the power spectrum\n",
    "    Pxx_norm = Pxx / Pxx.sum()\n",
    "    # Calculate the spectral entropy\n",
    "    se = -1 * (Pxx_norm * np.log2(Pxx_norm)).sum()\n",
    "    return se\n",
    "\n",
    "def hjorth_complexity(x):\n",
    "    dx = np.diff(x)\n",
    "    d2x = np.diff(dx)\n",
    "    var_x = np.var(x)\n",
    "    var_dx = np.var(dx)\n",
    "    var_d2x = np.var(d2x)\n",
    "    activity = var_x\n",
    "    mobility = np.sqrt(var_d2x / var_dx)\n",
    "    # Calculate Hjorth complexity\n",
    "    complexity = mobility / activity\n",
    "    return complexity\n",
    "    \n",
    "# Kurtosis = @(x) ((1/size(x,1))*sum((x - mean(x)).^4))./(((1/size(x,1))*sum((x - mean(x)).^2)).^2);\n",
    "def Kurtosis(x):\n",
    "    return ((1/x.shape[0])*np.sum((x - np.mean(x))**4))/(((1/x.shape[0])*np.sum((x - np.mean(x))**2))**2)\n",
    "\n",
    "def Covariance(x):\n",
    "    convar = np.cov(x, rowvar=False)\n",
    "    feat = []\n",
    "    for i in range(convar.shape[0]):\n",
    "        feat += [convar[i, :i+1]]\n",
    "    return np.concatenate(feat)\n",
    "\n",
    "def get_features(filtered_window, fs=1000):\n",
    "    \"\"\"\n",
    "        Write a function that calculates features for a given filtered window. \n",
    "        Feel free to use features you have seen before in this class, features that\n",
    "        have been used in the literature, or design your own!\n",
    "\n",
    "        Input: \n",
    "        filtered_window (window_samples x channels): the window of the filtered ecog signal \n",
    "        fs: sampling rate\n",
    "        Output:\n",
    "        features (channels x num_features): the features calculated on each channel for the window\n",
    "    \"\"\"\n",
    "    feat_LL = LineLength(filtered_window)\n",
    "    feat_Area = Area(filtered_window)\n",
    "    feat_Energy = Energy(filtered_window)\n",
    "    feat_ZCM = ZeroCrossingMean(filtered_window)\n",
    "    feat_TimeAvg = averageTimeDomain(filtered_window)\n",
    "#     feat_SpectralEntropy = spectral_entropy(filtered_window)\n",
    "    feat_Hijorth = hjorth_complexity(filtered_window)\n",
    "    feat_kurtosis = Kurtosis(filtered_window)\n",
    "    feat_covariance = Covariance(filtered_window)\n",
    "    # feat_FreqAvg = averageFreqDomain(filtered_window)\n",
    "    \n",
    "    from pyriemann.estimation import Covariances\n",
    "    from pyriemann.tangentspace import TangentSpace\n",
    "    \n",
    "    # covar = Covariances().fit_transform(np.expand_dims(filtered_window.T, 0))\n",
    "    # # covest = Covariances('oas')\n",
    "    # # temp = np.expand_dims(filtered_window, axis=-1)\n",
    "    # # covar = covest.fit_transform(temp)\n",
    "    # ts = TangentSpace()\n",
    "    # tsfeat = ts.fit_transform(covar)\n",
    "    # # print(tsfeat.shape)\n",
    "\n",
    "    # raise notImplementedError()\n",
    "    return np.hstack([#feat_LL, \n",
    "                      #feat_Area, \n",
    "                      feat_covariance,\n",
    "                      feat_Energy, \n",
    "                      #feat_ZCM, \n",
    "                      feat_TimeAvg, \n",
    "#                       feat_SpectralEntropy,\n",
    "                      feat_Hijorth,\n",
    "                      feat_kurtosis,\n",
    "                    #   feat_covariance,\n",
    "                      bandpower(filtered_window, 1000, 5, 15),\n",
    "                      bandpower(filtered_window, 1000, 20, 25),\n",
    "                      bandpower(filtered_window, 1000, 75, 115),\n",
    "                      bandpower(filtered_window, 1000, 125, 160),\n",
    "                      bandpower(filtered_window, 1000, 160, 175)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_feats(raw_ecog, fs, window_length, window_overlap):\n",
    "    \"\"\"\n",
    "        Write a function which processes data through the steps of filtering and\n",
    "        feature calculation and returns features. Points will be awarded for completing\n",
    "        each step appropriately (note that if one of the functions you call within this script\n",
    "        returns a bad output, you won't be double penalized). Note that you will need\n",
    "        to run the filter_data and get_features functions within this function. \n",
    "\n",
    "        Inputs:\n",
    "        raw_eeg (samples x channels): the raw signal\n",
    "        fs: the sampling rate (1000 for this dataset)\n",
    "        window_length: the window's length\n",
    "        window_overlap: the window's overlap\n",
    "        Output: \n",
    "        all_feats (num_windows x (channels x features)): the features for each channel for each time window\n",
    "            note that this is a 2D array. \n",
    "    \"\"\"\n",
    "    raw_ecog = filter_data(raw_ecog, fs)\n",
    "    \n",
    "    window_disp = window_length - window_overlap\n",
    "    \n",
    "    all_feats = np.vstack([get_features(raw_ecog[int(i * window_disp * fs):int(i * window_disp * fs + window_length * fs), :], fs) for i in range(NumWins(raw_ecog, fs, window_length, window_disp))])\n",
    "    \n",
    "    return all_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_R_matrix(features, N_wind):\n",
    "    \"\"\" \n",
    "    Write a function to calculate the R matrix\n",
    "\n",
    "    Input:\n",
    "        features (samples (number of windows in the signal) x channels x features): \n",
    "        the features you calculated using get_windowed_feats\n",
    "        N_wind: number of windows to use in the R matrix\n",
    "\n",
    "    Output:\n",
    "        R (samples x (N_wind*channels*features))\n",
    "    \"\"\"\n",
    "    num_win = features.shape[0]\n",
    "    num_channel_features = features.shape[1]\n",
    "    \n",
    "    # Append a copy of the first N-1 rows to the beginning of features\n",
    "    features = np.vstack((features[:N_wind-1], features))\n",
    "    \n",
    "    R = np.zeros((num_win, N_wind * num_channel_features))\n",
    "    \n",
    "    for i in range(num_win):\n",
    "        # Get the feature matrix for the current window\n",
    "        # Resize the feature matrix and store in R\n",
    "        R[i,:] = features[i:i+N_wind,:].reshape(-1)\n",
    "\n",
    "    R = np.hstack((np.ones((R.shape[0], 1)), R))\n",
    "\n",
    "    return R\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "winLen = 100 / 1e3\n",
    "winOverlap = 50 / 1e3\n",
    "winDisp = winLen - winOverlap\n",
    "\n",
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_1_train, 1000, winLen, winOverlap)\n",
    "R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_1_test, 1000, winLen, winOverlap)\n",
    "R_test = create_R_matrix(feature_test, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_1_train\n",
    "Y_test = data_glove_1_test\n",
    "Y_train = sig.resample(Y_train, R_train.shape[0], axis=0)\n",
    "Y_test = sig.resample(Y_test, R_test.shape[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/numpy/lib/function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "corre = np.corrcoef(feature_train, Y_train, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.25493011,  0.04593392, ..., -0.00276058,\n",
       "        -0.02351143, -0.08008796],\n",
       "       [ 0.25493011,  1.        ,  0.07340108, ..., -0.03964397,\n",
       "        -0.02167498, -0.02115068],\n",
       "       [ 0.04593392,  0.07340108,  1.        , ..., -0.02058149,\n",
       "        -0.01985489,  0.02815554],\n",
       "       ...,\n",
       "       [-0.00276058, -0.03964397, -0.02058149, ...,  1.        ,\n",
       "         0.09794208,  0.08317813],\n",
       "       [-0.02351143, -0.02167498, -0.01985489, ...,  0.09794208,\n",
       "         1.        , -0.12152057],\n",
       "       [-0.08008796, -0.02115068,  0.02815554, ...,  0.08317813,\n",
       "        -0.12152057,  1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan_to_num(corre, copy=False, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.argsort(corre[:-4, -4:].ravel())[-800:], corre[:-4, -4:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ = idx[0][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_ = np.unique(idx_)\n",
    "idx_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_ = feature_train[:, idx_]\n",
    "feature_test_ = feature_test[:, idx_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train = create_R_matrix(feature_train_, 5)\n",
    "R_test = create_R_matrix(feature_train_, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the weights\n",
    "# f_train = np.linalg.pinv(R_train.T @ R_train) @ (R_train.T @ Y_train)\n",
    "\n",
    "# prediction_LR = (R_test @ f_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4199, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m xgb_reg \u001b[38;5;241m=\u001b[39m XGBRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# fit model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mxgb_reg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# make predictions\u001b[39;00m\n\u001b[1;32m     10\u001b[0m prediction_XGB \u001b[38;5;241m=\u001b[39m xgb_reg\u001b[38;5;241m.\u001b[39mpredict(R_test)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.9/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "# read data\n",
    "\n",
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R_train, Y_train)\n",
    "# make predictions\n",
    "prediction_XGB = xgb_reg.predict(R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(prediction, target):\n",
    "    corr = [pearsonr(prediction[:,i], target[:,i]).statistic for i in range(4)]\n",
    "    return corr, np.mean(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Subject 1\n",
      "For XGBoost: ([0.28746230332385686, 0.358173949666523, 0.031368871377739554, 0.07395861025586921], 0.18774093365599717)\n"
     ]
    }
   ],
   "source": [
    "print('For Subject 1')\n",
    "# print(f'For linear regression: {correlation(prediction_LR, Y_test)}')\n",
    "print(f'For XGBoost: {correlation(prediction_XGB, Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_2_train, 1000, winLen, winOverlap)\n",
    "R_train = create_R_matrix(feature_train, 3)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_2_test, 1000, winLen, winOverlap)\n",
    "R_test = create_R_matrix(feature_test, 3)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_2_train\n",
    "Y_test = data_glove_2_test\n",
    "Y_train = sig.resample(Y_train, R_train.shape[0], axis=0)\n",
    "Y_test = sig.resample(Y_test, R_test.shape[0], axis=0)\n",
    "\n",
    "# Compute the weights\n",
    "f_train = np.linalg.pinv(R_train.T @ R_train) @ (R_train.T @ Y_train)\n",
    "prediction_LR = (R_test @ f_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=200, max_depth=5, max_leaf_nodes=200)\n",
    "rf_reg.fit(R_train, Y_train)\n",
    "prediction_RF = rf_reg.predict(R_test)\n",
    "\n",
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R_train, Y_train)\n",
    "# make predictions\n",
    "prediction_XGB = xgb_reg.predict(R_test)\n",
    "\n",
    "\n",
    "print('For Subject 2')\n",
    "print(f'For linear regression: {[pearsonr(prediction_LR[:,i], Y_test[:,i]).statistic for i in range(4)]}')\n",
    "print(f'For random forest: {[pearsonr(prediction_RF[:,i], Y_test[:,i]).statistic for i in range(4)]}')\n",
    "print(f'For XGBoost: {[pearsonr(prediction_XGB[:,i], Y_test[:,i]).statistic for i in range(4)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Subject 3\n",
      "For linear regression: [0.515557634501014, 0.2753656952491606, 0.3861286395818362, 0.39087312864622037]\n",
      "For random forest: [0.7243048760525532, 0.5249419449942999, 0.6476626372046566, 0.42160591268365893]\n",
      "For XGBoost: [0.7026111184226042, 0.4022655712154394, 0.6120286981874062, 0.4094253084822335]\n"
     ]
    }
   ],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_3_train, 1000, winLen, winOverlap)\n",
    "R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_3_test, 1000, winLen, winOverlap)\n",
    "R_test = create_R_matrix(feature_test, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_3_train\n",
    "Y_test = data_glove_3_test\n",
    "Y_train = sig.resample(Y_train, R_train.shape[0], axis=0)\n",
    "Y_test = sig.resample(Y_test, R_test.shape[0], axis=0)\n",
    "\n",
    "# Compute the weights\n",
    "f_train = np.linalg.pinv(R_train.T @ R_train) @ (R_train.T @ Y_train)\n",
    "prediction_LR = (R_test @ f_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=5, max_leaf_nodes=200)\n",
    "rf_reg.fit(R_train, Y_train)\n",
    "prediction_RF = rf_reg.predict(R_test)\n",
    "\n",
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R_train, Y_train)\n",
    "# make predictions\n",
    "prediction_XGB = xgb_reg.predict(R_test)\n",
    "\n",
    "print('For Subject 3')\n",
    "print(f'For linear regression: {[pearsonr(prediction_LR[:,i], Y_test[:,i]).statistic for i in range(4)]}')\n",
    "print(f'For random forest: {[pearsonr(prediction_RF[:,i], Y_test[:,i]).statistic for i in range(4)]}')\n",
    "print(f'For XGBoost: {[pearsonr(prediction_XGB[:,i], Y_test[:,i]).statistic for i in range(4)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_1, 1000, winLen, winOverlap)\n",
    "R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_1\n",
    "Y_train = sig.resample(Y_train, R_train.shape[0], axis=0)\n",
    "\n",
    "# Compute the weights\n",
    "f_train = np.linalg.pinv(R_train.T @ R_train) @ (R_train.T @ Y_train)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# rf_reg = RandomForestRegressor(n_estimators=100, max_depth=5, max_leaf_nodes=200)\n",
    "# rf_reg.fit(R_train, Y_train)\n",
    "\n",
    "\n",
    "# np.save('./models/LR_Matrix_S1', f_train)\n",
    "# pickle.dump(rf_reg, open('./models/RF_Matrix_S1.pth', 'wb'))\n",
    "\n",
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R_train, Y_train)\n",
    "# make predictions\n",
    "# prediction_XGB = xgb_reg.predict(R_test)\n",
    "\n",
    "\n",
    "xgb_reg.save_model('./models/XGB_S1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 48, using nperseg = 48\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    }
   ],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_2, 1000, winLen, winOverlap)\n",
    "R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_2\n",
    "Y_train = sig.resample(Y_train, R_train.shape[0], axis=0)\n",
    "\n",
    "# Compute the weights\n",
    "f_train = np.linalg.pinv(R_train.T @ R_train) @ (R_train.T @ Y_train)\n",
    "\n",
    "# from sklearn. ensemble import RandomForestRegressor\n",
    "# rf_reg = RandomForestRegressor(n_estimators=100, max_depth=5, max_leaf_nodes=200)\n",
    "# rf_reg.fit(R_train, Y_train)\n",
    "\n",
    "\n",
    "# np.save('./models/LR_Matrix_S2', f_train)\n",
    "# pickle.dump(rf_reg, open('./models/RF_Matrix_S2.pth', 'wb'))\n",
    "\n",
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R_train, Y_train)\n",
    "# make predictions\n",
    "# prediction_XGB = xgb_reg.predict(R_test)\n",
    "\n",
    "\n",
    "xgb_reg.save_model('./models/XGB_S2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 64, using nperseg = 64\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    }
   ],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_3, 1000, winLen, winOverlap)\n",
    "R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_3\n",
    "Y_train = sig.resample(Y_train, R_train.shape[0], axis=0)\n",
    "\n",
    "# Compute the weights\n",
    "f_train = np.linalg.pinv(R_train.T @ R_train) @ (R_train.T @ Y_train)\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# rf_reg = RandomForestRegressor(n_estimators=100, max_depth=5, max_leaf_nodes=200)\n",
    "# rf_reg.fit(R_train, Y_train)\n",
    "\n",
    "\n",
    "# np.save('./models/LR_Matrix_S3', f_train)\n",
    "# pickle.dump(rf_reg, open('./models/RF_Matrix_S3.pth', 'wb'))\n",
    "\n",
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R_train, Y_train)\n",
    "# make predictions\n",
    "# prediction_XGB = xgb_reg.predict(R_test)\n",
    "\n",
    "\n",
    "xgb_reg.save_model('./models/XGB_S3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5999, 3521)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
