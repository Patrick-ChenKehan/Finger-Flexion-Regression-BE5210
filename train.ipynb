{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Model Saving Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the notebook environment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import signal as sig\n",
    "from utils import *\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = scipy.io.loadmat('./datasets/raw_training_data.mat')\n",
    "data_glove_1 = raw['train_dg'][0][0]\n",
    "data_glove_1_train = np.delete(data_glove_1, 3, 1)\n",
    "data_glove_2 = raw['train_dg'][1][0]\n",
    "data_glove_2_train = np.delete(data_glove_2, 3, 1)\n",
    "data_glove_3 = raw['train_dg'][2][0]\n",
    "data_glove_3_train = np.delete(data_glove_3, 3, 1)\n",
    "\n",
    "ecog_1_train = raw['train_ecog'][0][0]\n",
    "ecog_2_train = raw['train_ecog'][1][0]\n",
    "ecog_3_train = raw['train_ecog'][2][0]\n",
    "\n",
    "\n",
    "raw = scipy.io.loadmat('./datasets/sub1_comp.mat')\n",
    "ecog_1_comp = raw['train_data']\n",
    "dg_1_comp = raw['train_dg']\n",
    "ecog_1_valid = raw['test_data'][49000:]\n",
    "\n",
    "raw = scipy.io.loadmat('./datasets/sub2_comp.mat')\n",
    "ecog_2_comp = raw['train_data']\n",
    "dg_2_comp = raw['train_dg']\n",
    "ecog_2_valid = raw['test_data'][49000:]\n",
    "\n",
    "raw = scipy.io.loadmat('./datasets/sub3_comp.mat')\n",
    "ecog_3_comp = raw['train_data']\n",
    "dg_3_comp = raw['train_dg']\n",
    "ecog_3_valid = raw['test_data'][49000:]\n",
    "\n",
    "dg_1_raw = scipy.io.loadmat('./datasets/sub1_testlabels.mat')\n",
    "dg_1_valid = dg_1_raw['test_dg'][49000:]\n",
    "dg_1_valid = np.delete(dg_1_valid, 3, 1)\n",
    "\n",
    "dg_2_raw = scipy.io.loadmat('./datasets/sub2_testlabels.mat')\n",
    "dg_2_valid = dg_2_raw['test_dg'][49000:]\n",
    "dg_2_valid = np.delete(dg_2_valid, 3, 1)\n",
    "\n",
    "dg_3_raw = scipy.io.loadmat('./datasets/sub3_testlabels.mat')\n",
    "dg_3_valid = dg_3_raw['test_dg'][49000:]\n",
    "dg_3_valid = np.delete(dg_3_valid, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute features and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "winLen = 100 / 1e3\n",
    "winOverlap = 50 / 1e3\n",
    "winDisp = winLen - winOverlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, winLen, winDisp):\n",
    "    result = []\n",
    "    for i in range(NumWins(x, 1000, winLen, winDisp)):\n",
    "        result.append(x[i * int(winDisp * 1000):i * int(winDisp * 1000) + int(winLen * 1000)].mean(axis=0))\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5999, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_average(data_glove_1_train, winLen, winDisp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_1_train, 1000, winLen, winOverlap)\n",
    "# R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_1_valid, 1000, winLen, winOverlap)\n",
    "# R_test = create_R_matrix(feature_test, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_1_train\n",
    "Y_test = dg_1_valid\n",
    "Y_train = moving_average(Y_train, winLen, winDisp)\n",
    "Y_test = moving_average(Y_test, winLen, winDisp)\n",
    "\n",
    "R = create_R_matrix(feature_train, 20)\n",
    "R_test = create_R_matrix(feature_test, 20)\n",
    "\n",
    "idx_1 = feature_selection(R, Y_train, 800)\n",
    "print(idx_1.shape)\n",
    "\n",
    "R = R[:, idx_1]\n",
    "R_test = R_test[:, idx_1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Subject 1\n",
      "For XGBoost: ([0.4739681823029282, 0.6634626449660738, 0.1590766612353765, 0.23771983478446282], 0.3835568308222103)\n"
     ]
    }
   ],
   "source": [
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=200, max_depth=20, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R, Y_train)\n",
    "# make predictions\n",
    "prediction_XGB_1 = xgb_reg.predict(R_test)\n",
    "\n",
    "print('For Subject 1')\n",
    "print(f'For XGBoost: {correlation(prediction_XGB_1, Y_test)}')\n",
    "\n",
    "xgb_reg.save_model('./models/XGB_S1.json')\n",
    "np.save('./models/idx_S1.npy', idx_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009935 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 149930\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 617\n",
      "[LightGBM] [Info] Start training from score -0.031320\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 149930\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 617\n",
      "[LightGBM] [Info] Start training from score -0.026964\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 149930\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 617\n",
      "[LightGBM] [Info] Start training from score -0.057710\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 149930\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 617\n",
      "[LightGBM] [Info] Start training from score -0.026658\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "For Subject 1\n",
      "For LightGBM: ([0.6398002068564572, 0.7567631413776371, 0.2828389712796067, 0.321590496246339], 0.50024820394001)\n",
      "For ensemble: ([0.599014929427659, 0.7387931487649391, 0.24790018543399758, 0.3054610390464396], 0.4727923256682588)\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbm_reg_list = [LGBMRegressor(n_estimators=1000, max_depth=20, learning_rate=0.01) for _ in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    lgbm_reg_list[i].fit(R, Y_train[:,i])\n",
    "    lgbm_reg_list[i].booster_.save_model(f'./models/lgbr_f{i}_S1.txt')\n",
    "\n",
    "prediction_lgbm_list = [lgbm_reg.predict(R_test) for lgbm_reg in lgbm_reg_list]\n",
    "prediction_lgbm_1 = np.vstack(prediction_lgbm_list).T\n",
    "print('For Subject 1')\n",
    "print(f'For LightGBM: {correlation(prediction_lgbm_1, Y_test)}')\n",
    "\n",
    "prediction_ensemble = (prediction_XGB_1 + prediction_lgbm_1) / 2\n",
    "print(f'For ensemble: {correlation(prediction_ensemble, Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 48, using nperseg = 48\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(551,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5999, 551)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winLen = 100 / 1e3\n",
    "winOverlap = 50 / 1e3\n",
    "winDisp = winLen - winOverlap\n",
    "\n",
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_2_train, 1000, winLen, winOverlap)\n",
    "# R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_2_valid, 1000, winLen, winOverlap)\n",
    "# R_test = create_R_matrix(feature_test, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_2_train\n",
    "Y_test = dg_2_valid\n",
    "Y_train = moving_average(Y_train, winLen, winDisp)\n",
    "Y_test = moving_average(Y_test, winLen, winDisp)\n",
    "# Y_train = sig.resample(Y_train, feature_train.shape[0], axis=0)\n",
    "# Y_test = sig.resample(Y_test, feature_test.shape[0], axis=0)\n",
    "\n",
    "\n",
    "R = create_R_matrix(feature_train, 20)\n",
    "R_test = create_R_matrix(feature_test, 20)\n",
    "\n",
    "idx_2 = feature_selection(R, Y_train, 800)\n",
    "print(idx_2.shape)\n",
    "\n",
    "R = R[:, idx_2]\n",
    "R_test = R_test[:, idx_2]\n",
    "\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Subject 2\n",
      "For XGBoost: ([0.6014648549363719, 0.3972725971419602, 0.24884421381154292, 0.24780762636223888], 0.3738473230630285)\n"
     ]
    }
   ],
   "source": [
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R, Y_train)\n",
    "# make predictions\n",
    "prediction_XGB = xgb_reg.predict(R_test)\n",
    "\n",
    "print('For Subject 2')\n",
    "print(f'For XGBoost: {correlation(prediction_XGB, Y_test)}')\n",
    "\n",
    "xgb_reg.save_model('./models/XGB_S2.json')\n",
    "np.save('./models/idx_S2.npy', idx_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 125585\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 551\n",
      "[LightGBM] [Info] Start training from score -0.150330\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 125585\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 551\n",
      "[LightGBM] [Info] Start training from score -0.115466\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 125585\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 551\n",
      "[LightGBM] [Info] Start training from score 0.104593\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008092 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 125585\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 551\n",
      "[LightGBM] [Info] Start training from score -0.015761\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "For Subject 2\n",
      "For LightGBM: ([0.5936923830158624, 0.4187297536135232, 0.2483092049458494, 0.23207140625443928], 0.3732006869574186)\n",
      "For ensemble: ([0.6039648250587205, 0.4163571442093698, 0.2529891904942399, 0.24991549342964833], 0.3808066632979946)\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbm_reg_list = [LGBMRegressor(n_estimators=1000, max_depth=20, learning_rate=0.001) for _ in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    lgbm_reg_list[i].fit(R, Y_train[:,i])\n",
    "    lgbm_reg_list[i].booster_.save_model(f'./models/lgbr_f{i}_S2.txt')\n",
    "\n",
    "prediction_lgbm_list = [lgbm_reg.predict(R_test) for lgbm_reg in lgbm_reg_list]\n",
    "prediction_lgbm_2 = np.vstack(prediction_lgbm_list).T\n",
    "print('For Subject 2')\n",
    "print(f'For LightGBM: {correlation(prediction_lgbm_2, Y_test)}')\n",
    "\n",
    "prediction_ensemble = (prediction_XGB + prediction_lgbm_2) / 2\n",
    "print(f'For ensemble: {correlation(prediction_ensemble, Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 64, using nperseg = 64\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(597,)\n",
      "For Subject 3\n",
      "For XGBoost: ([0.7754575208444133, 0.6365313068913252, 0.6105067438980203, 0.6916622288288553], 0.6785394501156536)\n"
     ]
    }
   ],
   "source": [
    "# Compute the R matrix for the training data\n",
    "feature_train = get_windowed_feats(ecog_3_train, 1000, winLen, winOverlap)\n",
    "# R_train = create_R_matrix(feature_train, 5)\n",
    "\n",
    "feature_test = get_windowed_feats(ecog_3_valid, 1000, winLen, winOverlap)\n",
    "# R_test = create_R_matrix(feature_test, 5)\n",
    "\n",
    "# Downsample the glove data\n",
    "Y_train = data_glove_3_train\n",
    "Y_test = dg_3_valid\n",
    "Y_train = moving_average(Y_train, winLen, winDisp)\n",
    "Y_test = moving_average(Y_test, winLen, winDisp)\n",
    "# Y_train = sig.resample(Y_train, feature_train.shape[0], axis=0)\n",
    "# Y_test = sig.resample(Y_test, feature_test.shape[0], axis=0)\n",
    "\n",
    "R = create_R_matrix(feature_train, 20)\n",
    "R_test = create_R_matrix(feature_test, 20)\n",
    "\n",
    "idx_3 = feature_selection(R, Y_train, 800)\n",
    "print(idx_3.shape)\n",
    "\n",
    "R = R[:, idx_3]\n",
    "R_test = R_test[:, idx_3]\n",
    "\n",
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.01)\n",
    "# fit model\n",
    "xgb_reg.fit(R, Y_train)\n",
    "# make predictions\n",
    "prediction_XGB = xgb_reg.predict(R_test)\n",
    "\n",
    "print('For Subject 3')\n",
    "print(f'For XGBoost: {correlation(prediction_XGB, Y_test)}')\n",
    "\n",
    "xgb_reg.save_model('./models/XGB_S3.json')\n",
    "np.save('./models/idx_S3.npy', idx_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150111\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 597\n",
      "[LightGBM] [Info] Start training from score 0.081512\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150111\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 597\n",
      "[LightGBM] [Info] Start training from score 0.037428\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150111\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 597\n",
      "[LightGBM] [Info] Start training from score 0.016610\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 150111\n",
      "[LightGBM] [Info] Number of data points in the train set: 5999, number of used features: 597\n",
      "[LightGBM] [Info] Start training from score -0.065185\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "For Subject 3\n",
      "For LightGBM: ([0.787270593899093, 0.674705321341126, 0.618829770350158, 0.7433007580860886], 0.7060266109191164)\n",
      "For ensemble: ([0.7873718242212926, 0.6663353925629812, 0.6196122916068828, 0.7307150648428895], 0.7010086433085114)\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbm_reg_list = [LGBMRegressor(n_estimators=1000, max_depth=20, learning_rate=0.01) for _ in range(4)]\n",
    "\n",
    "for i in range(4):\n",
    "    lgbm_reg_list[i].fit(R, Y_train[:,i])\n",
    "    lgbm_reg_list[i].booster_.save_model(f'./models/lgbr_f{i}_S3.txt')\n",
    "\n",
    "prediction_lgbm_list = [lgbm_reg.predict(R_test) for lgbm_reg in lgbm_reg_list]\n",
    "prediction_lgbm_3 = np.vstack(prediction_lgbm_list).T\n",
    "print('For Subject 3')\n",
    "print(f'For LightGBM: {correlation(prediction_lgbm_3, Y_test)}')\n",
    "\n",
    "prediction_ensemble = (prediction_XGB + prediction_lgbm_3) / 2\n",
    "print(f'For ensemble: {correlation(prediction_ensemble, Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 48, using nperseg = 48\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n",
      "/usr/local/lib/python3.11/site-packages/scipy/signal/_spectral_py.py:2014: UserWarning: nperseg = 256 is greater than input length  = 64, using nperseg = 64\n",
      "  warnings.warn('nperseg = {0:d} is greater than input length '\n"
     ]
    }
   ],
   "source": [
    "ecog_1_leaderboard = ecog_1_comp[500: 500 + 147500]\n",
    "dg_1_leaderboard = dg_1_comp[500: 500 + 147500]\n",
    "\n",
    "ecog_2_leaderboard = ecog_2_comp[500: 500 + 147500]\n",
    "dg_2_leaderboard = dg_2_comp[500: 500 + 147500]\n",
    "\n",
    "ecog_3_leaderboard = ecog_3_comp[500: 500 + 147500]\n",
    "dg_3_leaderboard = dg_3_comp[500: 500 + 147500]\n",
    "\n",
    "dg_1_leaderboard = np.delete(dg_1_leaderboard, 3, 1)\n",
    "dg_2_leaderboard = np.delete(dg_2_leaderboard, 3, 1)\n",
    "dg_3_leaderboard = np.delete(dg_3_leaderboard, 3, 1)\n",
    "\n",
    "\n",
    "winLen = 100 / 1e3\n",
    "winOverlap = 50 / 1e3\n",
    "winDisp = winLen - winOverlap\n",
    "\n",
    "\n",
    "feature_1 = get_windowed_feats(ecog_1_leaderboard, 1000, winLen, winOverlap)\n",
    "# R_1 = create_R_matrix(feature_1, 5)\n",
    "feature_2 = get_windowed_feats(ecog_2_leaderboard, 1000, winLen, winOverlap)\n",
    "# R_2 = create_R_matrix(feature_2, 5)\n",
    "feature_3 = get_windowed_feats(ecog_3_leaderboard, 1000, winLen, winOverlap)\n",
    "# R_3 = create_R_matrix(feature_3, 5)\n",
    "\n",
    "idx_1 = np.load('./models/idx_S1.npy')\n",
    "idx_2 = np.load('./models/idx_S2.npy')\n",
    "idx_3 = np.load('./models/idx_S3.npy')\n",
    "\n",
    "R_1 = create_R_matrix(feature_1, 20)[:, idx_1]\n",
    "R_2 = create_R_matrix(feature_2, 20)[:, idx_2]\n",
    "R_3 = create_R_matrix(feature_3, 20)[:, idx_3]\n",
    "\n",
    "R_list = [R_1, R_2, R_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "predictions = []\n",
    "for i in range(3):\n",
    "    # Load XGB\n",
    "    xgb_reg = xgb.XGBRegressor()\n",
    "    xgb_reg.load_model(f\"./models/XGB_S{i + 1}.json\")\n",
    "\n",
    "    prediction_xgb = xgb_reg.predict(R_list[i])\n",
    "    \n",
    "    # Load LGBM\n",
    "    lgbm_reg_list = [lightgbm.Booster(model_file=f'./models/lgbr_f{j}_S{i + 1}.txt') for j in range(4)]\n",
    "    \n",
    "    prediction_lgbm_list = [lgbm_reg.predict(R_list[i]) for lgbm_reg in lgbm_reg_list]\n",
    "    prediction_lgbm = np.vstack(prediction_lgbm_list).T\n",
    "    \n",
    "    \n",
    "    prediction = prediction_lgbm\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Subject 1\n",
      "For XGBoost: ([0.5907641612580329, 0.7977617513623582, 0.4600879272260482, 0.27465960273210904], 0.530818360644637)\n",
      "For Subject 2\n",
      "For XGBoost: ([0.6250923349801124, 0.5283428967113556, 0.45170794050619206, 0.42148679872941325], 0.5066574927317683)\n",
      "For Subject 3\n",
      "For XGBoost: ([0.7403539510044402, 0.7341086861577378, 0.6143714085152925, 0.7867964007526729], 0.7189076116075359)\n"
     ]
    }
   ],
   "source": [
    "print('For Subject 1')\n",
    "print(f'For XGBoost: {correlation(sig.resample(predictions[0], dg_1_leaderboard.shape[0]), dg_1_leaderboard)}')\n",
    "\n",
    "print('For Subject 2')\n",
    "print(f'For XGBoost: {correlation(sig.resample(predictions[1], dg_2_leaderboard.shape[0]), dg_2_leaderboard)}')\n",
    "\n",
    "print('For Subject 3')\n",
    "print(f'For XGBoost: {correlation(sig.resample(predictions[2], dg_3_leaderboard.shape[0]), dg_3_leaderboard)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = sig.resample(predictions[0], dg_1_leaderboard.shape[0])\n",
    "p2 = sig.resample(predictions[1], dg_2_leaderboard.shape[0])\n",
    "p3 = sig.resample(predictions[2], dg_3_leaderboard.shape[0])\n",
    "\n",
    "p = np.concatenate((p1, p2, p3), axis=0)\n",
    "dg = np.concatenate((dg_1_leaderboard, dg_2_leaderboard, dg_3_leaderboard), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = scipy.io.loadmat('leaderboard_prediction.mat')['predicted_dg']\n",
    "prediction1 = predictions[0][0][:, [0,1,2,4]]\n",
    "prediction2 = predictions[1][0][:, [0,1,2,4]]\n",
    "prediction3 = predictions[2][0][:, [0,1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25594257, -0.2087674 , -0.0428858 , -0.13051725],\n",
       "       [-0.25594257, -0.2087674 , -0.0428858 , -0.13051725],\n",
       "       [-0.25594257, -0.2087674 , -0.0428858 , -0.13051725],\n",
       "       ...,\n",
       "       [-0.38377505,  0.06773657, -0.01280759,  1.42021552],\n",
       "       [-0.38377505,  0.06773657, -0.01280759,  1.42021552],\n",
       "       [-0.38377505,  0.06773657, -0.01280759,  1.42021552]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][0][:, [0,1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Subject 1\n",
      "For XGBoost: ([0.6597119562392924, 0.8263029671949991, 0.5359963726003356, 0.3729523962118694], 0.5987409230616241)\n",
      "For Subject 2\n",
      "For XGBoost: ([0.7069395067689479, 0.6064309370067075, 0.6143715293495223, 0.6165457399913061], 0.636071928279121)\n",
      "For Subject 3\n",
      "For XGBoost: ([0.8027001923945973, 0.7969405443623298, 0.6613162078526109, 0.8198776673989427], 0.7702086530021202)\n"
     ]
    }
   ],
   "source": [
    "print('For Subject 1')\n",
    "print(f'For XGBoost: {correlation(prediction1, dg_1_leaderboard)}')\n",
    "\n",
    "print('For Subject 2')\n",
    "print(f'For XGBoost: {correlation(prediction2, dg_2_leaderboard)}')\n",
    "\n",
    "print('For Subject 3')\n",
    "print(f'For XGBoost: {correlation(prediction3, dg_3_leaderboard)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x168869e90>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "import lightgbm\n",
    "lightgbm.Booster(model_file=f'./models/lgbr_f{0}_S{1}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6683405014476218"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.5987409230616241 +  0.636071928279121 + 0.7702086530021202) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
