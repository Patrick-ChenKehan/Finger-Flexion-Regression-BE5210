{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m roc_auc_score, precision_score, recall_score, accuracy_score\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.signal as sig\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = scipy.io.loadmat('./datasets/raw_training_data.mat')\n",
    "data_glove_1 = raw['train_dg'][0][0]\n",
    "data_glove_1 = np.delete(data_glove_1, 3, 1)\n",
    "data_glove_2 = raw['train_dg'][1][0]\n",
    "data_glove_2 = np.delete(data_glove_2, 3, 1)\n",
    "data_glove_3 = raw['train_dg'][2][0]\n",
    "data_glove_3 = np.delete(data_glove_3, 3, 1)\n",
    "\n",
    "ecog_1 = raw['train_ecog'][0][0]\n",
    "ecog_2 = raw['train_ecog'][1][0]\n",
    "ecog_3 = raw['train_ecog'][2][0]\n",
    "\n",
    "labels_1 = np.argmax(data_glove_1, axis=1)\n",
    "labels_2 = np.argmax(data_glove_2, axis=1)\n",
    "labels_3 = np.argmax(data_glove_3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(raw_eeg, fs=1000):\n",
    "    \"\"\"\n",
    "    Write a filter function to clean underlying data.\n",
    "    Filter type and parameters are up to you. Points will be awarded for reasonable filter type, parameters and application.\n",
    "    Please note there are many acceptable answers, but make sure you aren't throwing out crucial data or adversly\n",
    "    distorting the underlying data!\n",
    "\n",
    "    Input: \n",
    "        raw_eeg (samples x channels): the raw signal\n",
    "        fs: the sampling rate (1000 for this dataset)\n",
    "    Output: \n",
    "        clean_data (samples x channels): the filtered signal\n",
    "    \"\"\"\n",
    "    # dim = 100\n",
    "    # b = sig.firwin(numtaps=dim + 1, cutoff=[0.15, 200], pass_zero='bandpass', fs=fs)\n",
    "    b, a = sig.butter(N=2, Wn=[0.15, 200], btype='bandpass', fs=fs, output='ba')\n",
    "\n",
    "    filtered_eeg = sig.filtfilt(b, a, x=raw_eeg, axis=0)\n",
    "    \n",
    "    return filtered_eeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_1 = filter_data(ecog_1)\n",
    "ecog_2 = filter_data(ecog_2)\n",
    "ecog_3 = filter_data(ecog_3)\n",
    "\n",
    "train_test_ratio = 0.7\n",
    "\n",
    "ecog_1_train = ecog_1[:int(train_test_ratio * ecog_1.shape[0])]\n",
    "ecog_1_test = ecog_1[int(train_test_ratio * ecog_1.shape[0]):]\n",
    "data_glove_1_train = data_glove_1[:int(train_test_ratio * data_glove_1.shape[0])]\n",
    "data_glove_1_test = data_glove_1[int(train_test_ratio * data_glove_1.shape[0]):]\n",
    "\n",
    "ecog_2_train = ecog_2[:int(train_test_ratio * ecog_2.shape[0])]\n",
    "ecog_2_test = ecog_2[int(train_test_ratio * ecog_2.shape[0]):]\n",
    "data_glove_2_train = data_glove_2[:int(train_test_ratio * data_glove_2.shape[0])]\n",
    "data_glove_2_test = data_glove_2[int(train_test_ratio * data_glove_2.shape[0]):]\n",
    "\n",
    "ecog_3_train = ecog_3[:int(train_test_ratio * ecog_3.shape[0])]\n",
    "ecog_3_test = ecog_3[int(train_test_ratio * ecog_3.shape[0]):]\n",
    "data_glove_3_train = data_glove_3[:int(train_test_ratio * data_glove_3.shape[0])]\n",
    "data_glove_3_test = data_glove_3[int(train_test_ratio * data_glove_3.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_glove_1_train.reshape(-1, 200, 4).mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 2000, 4)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arrange ecog_1_train into 2000 points windows with stride 500\n",
    "ecog_1_train_new = np.array([ecog_1_train[i:i+2000] for i in range(0, ecog_1_train.shape[0] - 2000, 500)])\n",
    "data_glove_1_train_new = np.array([data_glove_1_train[i:i+2000] for i in range(0, data_glove_1_train.shape[0] - 2000, 500)])\n",
    "\n",
    "\n",
    "ecog_1_valid_new = np.array([ecog_1_test[i:i+2000] for i in range(2000, ecog_1_test.shape[0] - 2000, 500)])\n",
    "data_glove_1_valid_new = np.array([data_glove_1_test[i:i+2000] for i in range(2000, data_glove_1_test.shape[0] - 2000, 500)])\n",
    "data_glove_1_valid_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerDataset(Dataset):\n",
    "    def __init__(self, ecog, dg, window=2000):\n",
    "        self.ecog = np.float32(ecog.reshape(-1, 1, window, ecog.shape[1]))\n",
    "        self.ecog = (self.ecog - self.ecog.mean()) / self.ecog.std()\n",
    "        self.dg = np.float32(dg.reshape(-1, window, 4).mean(axis=1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecog)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # r = r[:, np.random.permutation(r.shape[1])]\n",
    "        r = self.ecog[idx]\n",
    "        r = r[:,:, np.random.permutation(r.shape[2])]\n",
    "        return r, self.dg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerDataset(Dataset):\n",
    "    def __init__(self, ecog, dg, window=2000):\n",
    "        self.ecog = np.float32(ecog.reshape(ecog.shape[0], 1, ecog.shape[1], ecog.shape[2]))\n",
    "        self.ecog = (self.ecog - self.ecog.mean()) / self.ecog.std()\n",
    "        self.dg = np.float32(dg.mean(axis=1)).argmax(axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecog)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ecog[idx], self.dg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s1_train = FingerDataset(ecog_1_train.copy(), data_glove_1_train.copy())\n",
    "dataset_s1_valid = FingerDataset(ecog_1_test.copy(), data_glove_1_test.copy())\n",
    "\n",
    "train_loader = DataLoader(dataset_s1_train, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(dataset_s1_valid, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.28872958, -0.39639145,  0.18533479, ..., -0.09954405,\n",
       "          0.4653647 , -0.2981479 ],\n",
       "        [-0.3409584 , -0.4518811 ,  0.11538914, ..., -0.14991836,\n",
       "          0.38839257, -0.2993777 ],\n",
       "        [-0.41199115, -0.51223487,  0.0390866 , ..., -0.21089442,\n",
       "          0.3024675 , -0.2975337 ],\n",
       "        ...,\n",
       "        [ 0.65357035, -0.36680967, -0.1312886 , ..., -0.23567398,\n",
       "          0.06565088, -0.2011317 ],\n",
       "        [ 0.65955096, -0.3942174 , -0.13500713, ..., -0.2441146 ,\n",
       "          0.04929653, -0.21758616],\n",
       "        [ 0.66925377, -0.42243156, -0.13053615, ..., -0.24147059,\n",
       "          0.03689422, -0.22789973]]], dtype=float32)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_s1_train[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 100,  20],\n",
       "       [  2, 401,  31],\n",
       "       [  8, 108,  11]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.array([[  1,  20, 100],\n",
    "       [  2,  31, 401],\n",
    "       [  8,  11, 108]])\n",
    "\n",
    "r = r[:, np.random.permutation(r.shape[1])]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1,62), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        #self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (8, 8))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 144 timepoints. \n",
    "        #4, 2, 487\n",
    "        self.fc1 = nn.Linear(132, 64) #4,1,10\n",
    "        self.fc2 = nn.Linear(64, 4) #4,1,10\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "      #  x = x.double()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "        # print (x.size())\n",
    "        # FC Layer\n",
    "        x = torch.flatten(x, start_dim=1)# 4*2*9) # for T=128\n",
    "        #x = x.view(-1,4 *2 * 9)# 4*2*9) # for T=128\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "\n",
    "class Conv2dWithConstraint(nn.Conv2d):\n",
    "    def __init__(self, *args, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        super(Conv2dWithConstraint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "        )\n",
    "        return super(Conv2dWithConstraint, self).forward(x)\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def InitialBlocks(self, dropoutRate, *args, **kwargs):\n",
    "        block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.F1, (1, self.kernelLength), stride=1, padding=(0, self.kernelLength // 2), bias=False),\n",
    "            nn.BatchNorm2d(self.F1, momentum=0.01, affine=True, eps=1e-3),\n",
    "\n",
    "            # DepthwiseConv2D =======================\n",
    "            Conv2dWithConstraint(self.F1, self.F1 * self.D, (self.channels, 1), max_norm=1, stride=1, padding=(0, 0),\n",
    "                                 groups=self.F1, bias=False),\n",
    "            # ========================================\n",
    "\n",
    "            nn.BatchNorm2d(self.F1 * self.D, momentum=0.01, affine=True, eps=1e-3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4), stride=4),\n",
    "            nn.Dropout(p=dropoutRate))\n",
    "        block2 = nn.Sequential(\n",
    "            # SeparableConv2D =======================\n",
    "            nn.Conv2d(self.F1 * self.D, self.F1 * self.D, (1, self.kernelLength2), stride=1,\n",
    "                      padding=(0, self.kernelLength2 // 2), bias=False, groups=self.F1 * self.D),\n",
    "            nn.Conv2d(self.F1 * self.D, self.F2, 1, padding=(0, 0), groups=1, bias=False, stride=1),\n",
    "            # ========================================\n",
    "\n",
    "            nn.BatchNorm2d(self.F2, momentum=0.01, affine=True, eps=1e-3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8), stride=8),\n",
    "            nn.Dropout(p=dropoutRate))\n",
    "        return nn.Sequential(block1, block2)\n",
    "\n",
    "\n",
    "    def ClassifierBlock(self, inputSize, n_classes):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(1952, n_classes, bias=False),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def CalculateOutSize(self, model, channels, samples):\n",
    "        '''\n",
    "        Calculate the output based on input size.\n",
    "        model is from nn.Module and inputSize is a array.\n",
    "        '''\n",
    "        data = torch.rand(1, 1, channels, samples)\n",
    "        model.eval()\n",
    "        out = model(data).shape\n",
    "        return out[2:]\n",
    "\n",
    "    def __init__(self, n_classes=4, channels=60, samples=151,\n",
    "                 dropoutRate=0.5, kernelLength=64, kernelLength2=16, F1=8,\n",
    "                 D=2, F2=16):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.F1 = F1\n",
    "        self.F2 = F2\n",
    "        self.D = D\n",
    "        self.samples = samples\n",
    "        self.n_classes = n_classes\n",
    "        self.channels = channels\n",
    "        self.kernelLength = kernelLength\n",
    "        self.kernelLength2 = kernelLength2\n",
    "        self.dropoutRate = dropoutRate\n",
    "\n",
    "        self.blocks = self.InitialBlocks(dropoutRate)\n",
    "        self.blockOutputSize = self.CalculateOutSize(self.blocks, channels, samples)\n",
    "        self.classifierBlock = self.ClassifierBlock(self.F2 * self.blockOutputSize[1], n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = x.view(x.size()[0], -1)  # Flatten\n",
    "        x = self.classifierBlock(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def categorical_cross_entropy(y_pred, y_true):\n",
    "    # y_pred = y_pred.cuda()\n",
    "    # y_true = y_true.cuda()\n",
    "    y_pred = torch.clamp(y_pred, 1e-9, 1 - 1e-9)\n",
    "    return -(y_true * torch.log(y_pred)).sum(dim=1).mean()\n",
    "\n",
    "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recursively to get params and weights\n",
    "        if type(module) in [\n",
    "            torch.nn.modules.container.Container,\n",
    "            torch.nn.modules.container.Sequential\n",
    "        ]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "\n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "        tmpstr += '  (' + key + '): ' + modstr\n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        if show_parameters:\n",
    "            tmpstr +=  ', parameters={}'.format(params)\n",
    "        tmpstr += '\\n'\n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    return tmpstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1,62), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        # self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        # self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        # self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        # self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        #4, 2, 487\n",
    "        self.fc1 = nn.Linear(8016, 256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "      #  x = x.double()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        # x = self.padding2(x)\n",
    "        # x = F.elu(self.conv3(x))\n",
    "        # x = self.batchnorm3(x)\n",
    "        # x = F.dropout(x, 0.25)\n",
    "        # x = self.pooling3(x)\n",
    "        # print (x.size())\n",
    "        # FC Layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = EEGNet()#.cuda(0)\n",
    "#print (net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)))))#.cuda(0))))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=0.01, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss: 21.505 | Train Cor: -0.087 | Valid loss: 2.971 | Valid Cor: -0.05184911596521971\n",
      "Epoch 2 | Train loss: 1.949 | Train Cor: 0.055 | Valid loss: 0.799 | Valid Cor: -0.01102028937899473\n",
      "Epoch 3 | Train loss: 0.778 | Train Cor: -0.121 | Valid loss: 0.669 | Valid Cor: 0.027125290769804113\n",
      "Epoch 4 | Train loss: 0.577 | Train Cor: 0.119 | Valid loss: 0.619 | Valid Cor: -0.0037869722863958233\n",
      "Epoch 5 | Train loss: 0.512 | Train Cor: 0.047 | Valid loss: 0.570 | Valid Cor: 0.058404663872196325\n",
      "Epoch 6 | Train loss: 0.456 | Train Cor: 0.259 | Valid loss: 0.530 | Valid Cor: 0.055863685162193935\n",
      "Epoch 7 | Train loss: 0.439 | Train Cor: 0.177 | Valid loss: 0.526 | Valid Cor: 0.1364335562479656\n",
      "Epoch 8 | Train loss: 0.439 | Train Cor: 0.211 | Valid loss: 0.543 | Valid Cor: 0.05547492846314258\n",
      "Epoch 9 | Train loss: 0.428 | Train Cor: 0.213 | Valid loss: 0.528 | Valid Cor: 0.06287463884047424\n",
      "Epoch 10 | Train loss: 0.426 | Train Cor: 0.137 | Valid loss: 0.565 | Valid Cor: 0.025201226267766504\n",
      "Epoch 11 | Train loss: 0.423 | Train Cor: 0.204 | Valid loss: 0.539 | Valid Cor: 0.045821298331412505\n",
      "Epoch 12 | Train loss: 0.382 | Train Cor: 0.237 | Valid loss: 0.560 | Valid Cor: 0.18114047043433443\n",
      "Epoch 13 | Train loss: 0.365 | Train Cor: 0.254 | Valid loss: 0.531 | Valid Cor: 0.16411198429054585\n",
      "Epoch 14 | Train loss: 0.375 | Train Cor: 0.208 | Valid loss: 0.593 | Valid Cor: 0.06876441737351818\n",
      "Epoch 15 | Train loss: 0.365 | Train Cor: 0.250 | Valid loss: 0.582 | Valid Cor: 0.13341167986693714\n",
      "Epoch 16 | Train loss: 0.360 | Train Cor: 0.268 | Valid loss: 0.580 | Valid Cor: 0.0928809417164433\n",
      "Epoch 17 | Train loss: 0.375 | Train Cor: 0.242 | Valid loss: 0.645 | Valid Cor: -0.08398475458655462\n",
      "Epoch 18 | Train loss: 0.351 | Train Cor: 0.307 | Valid loss: 0.608 | Valid Cor: -0.07127275220776778\n",
      "Epoch 19 | Train loss: 0.357 | Train Cor: 0.293 | Valid loss: 0.636 | Valid Cor: -0.03827081774752945\n",
      "Epoch 20 | Train loss: 0.350 | Train Cor: 0.297 | Valid loss: 0.538 | Valid Cor: 0.20647446641562722\n",
      "Epoch 21 | Train loss: 0.317 | Train Cor: 0.314 | Valid loss: 0.562 | Valid Cor: 0.09500668910553399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/kt/_vj_7fdx48j8b6r94gh03ktr0000gn/T/ipykernel_29878/979496279.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# print (loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = EEGNet() #.cuda(0)\n",
    "#print (net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)))))#.cuda(0))))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "net.train()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for (i, (ecog, dg)) in enumerate(train_loader):\n",
    "        output = net(ecog)\n",
    "        pred += [output.detach().numpy()]\n",
    "        loss = criterion(output, dg)\n",
    "        # print (loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # _, predicted = torch.max(output.data, 1)\n",
    "        #     # print(predicted, dg)\n",
    "        # total += dg.shape[0]\n",
    "        # correct += (predicted == dg).sum().item()\n",
    "    pred = np.concatenate(pred)\n",
    "    train_cor = pearsonr(np.repeat(pred, 2000, axis=0)[:,0], data_glove_1_train[:,0]).statistic\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    # train_acc = correct / total\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred = []\n",
    "        for (i, (ecog, dg)) in enumerate(test_loader):\n",
    "            output = net(ecog)\n",
    "            pred += [output.detach().numpy()]\n",
    "            loss = criterion(output, dg)\n",
    "            running_loss += loss.item()\n",
    "            # _, predicted = torch.max(output.data, 1)\n",
    "            # print(predicted, dg)\n",
    "            # total += dg.shape[0]\n",
    "            # correct += (predicted == dg).sum().item()\n",
    "        pred = np.concatenate(pred)\n",
    "        val_cor = pearsonr(np.repeat(pred, 2000, axis=0)[:,0], data_glove_1_test[:,0]).statistic\n",
    "        valid_loss = running_loss / len(test_loader)\n",
    "    # print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Acc: {train_acc:.3f} | Valid loss: {valid_loss:.3f} | Valid Acc: {val_cor}')\n",
    "    print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Cor: {train_cor:.3f} | Valid loss: {valid_loss:.3f} | Valid Cor: {val_cor}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False,  True, False])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(output.data, 1)[1] == dg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 4)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(pred, 500, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0491939338496332"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(np.repeat(pred, 500, axis=0)[:,0], data_glove_1_test[:,0]).statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
