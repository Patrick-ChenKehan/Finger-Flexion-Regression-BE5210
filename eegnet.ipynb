{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.signal as sig\n",
    "from scipy.stats import pearsonr\n",
    "from utils import *\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = scipy.io.loadmat('./datasets/raw_training_data.mat')\n",
    "data_glove_1 = raw['train_dg'][0][0]\n",
    "data_glove_1 = np.delete(data_glove_1, 3, 1)\n",
    "data_glove_2 = raw['train_dg'][1][0]\n",
    "data_glove_2 = np.delete(data_glove_2, 3, 1)\n",
    "data_glove_3 = raw['train_dg'][2][0]\n",
    "data_glove_3 = np.delete(data_glove_3, 3, 1)\n",
    "\n",
    "ecog_1 = raw['train_ecog'][0][0]\n",
    "ecog_2 = raw['train_ecog'][1][0]\n",
    "ecog_3 = raw['train_ecog'][2][0]\n",
    "\n",
    "labels_1 = np.argmax(data_glove_1, axis=1)\n",
    "labels_2 = np.argmax(data_glove_2, axis=1)\n",
    "labels_3 = np.argmax(data_glove_3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecog_1 = filter_data(ecog_1)\n",
    "ecog_2 = filter_data(ecog_2)\n",
    "ecog_3 = filter_data(ecog_3)\n",
    "\n",
    "train_test_ratio = 0.7\n",
    "\n",
    "ecog_1_train = ecog_1[:int(train_test_ratio * ecog_1.shape[0])]\n",
    "ecog_1_test = ecog_1[int(train_test_ratio * ecog_1.shape[0]):]\n",
    "data_glove_1_train = data_glove_1[:int(train_test_ratio * data_glove_1.shape[0])]\n",
    "data_glove_1_test = data_glove_1[int(train_test_ratio * data_glove_1.shape[0]):]\n",
    "\n",
    "ecog_2_train = ecog_2[:int(train_test_ratio * ecog_2.shape[0])]\n",
    "ecog_2_test = ecog_2[int(train_test_ratio * ecog_2.shape[0]):]\n",
    "data_glove_2_train = data_glove_2[:int(train_test_ratio * data_glove_2.shape[0])]\n",
    "data_glove_2_test = data_glove_2[int(train_test_ratio * data_glove_2.shape[0]):]\n",
    "\n",
    "ecog_3_train = ecog_3[:int(train_test_ratio * ecog_3.shape[0])]\n",
    "ecog_3_test = ecog_3[int(train_test_ratio * ecog_3.shape[0]):]\n",
    "data_glove_3_train = data_glove_3[:int(train_test_ratio * data_glove_3.shape[0])]\n",
    "data_glove_3_test = data_glove_3[int(train_test_ratio * data_glove_3.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 200\n",
    "X_train = ecog_1_train.reshape(-1, window, ecog_1_train.shape[1])\n",
    "y_train = data_glove_1_train.reshape(-1, window, data_glove_1_train.shape[1]).mean(axis=1)\n",
    "X_valid = ecog_1_test.reshape(-1, window, ecog_1_test.shape[1])\n",
    "y_valid = data_glove_1_test.reshape(-1, window, data_glove_1_test.shape[1]).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 200, 62)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RiemannFeatures(X):\n",
    "    Covariance = Covariances('oas')\n",
    "    covar = Covariance.fit_transform(np.transpose(X, (0, 2, 1)))\n",
    "    ts = TangentSpace()\n",
    "    tsfeat = ts.fit_transform(covar)\n",
    "    return tsfeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(X):\n",
    "    # Compute Spectrum features\n",
    "    feat_LL = LineLength(X)\n",
    "    feat_Area = Area(X)\n",
    "    feat_Energy = Energy(X)\n",
    "    feat_ZCM = ZeroCrossingMean(X)\n",
    "    feat_TimeAvg = averageTimeDomain(X)\n",
    "    feat_Riemann = RiemannFeatures(X)\n",
    "    return np.hstack([feat_Area, feat_Energy,\n",
    "                      feat_TimeAvg,\n",
    "                      feat_Riemann,\n",
    "                      BandPower(X, 1000, 5, 15),\n",
    "                      BandPower(X, 1000, 20, 25),\n",
    "                      BandPower(X, 1000, 75, 115),\n",
    "                      BandPower(X, 1000, 125, 160),\n",
    "                      BandPower(X, 1000, 160, 175)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "# temp = np.transpose(X_train, (0, 2, 1))\n",
    "Covariance = Covariances('oas')\n",
    "covar_train = Covariance.fit_transform(np.transpose(X_train, (0, 2, 1)))\n",
    "covar_valid = Covariance.transform(np.transpose(X_valid, (0, 2, 1)))\n",
    "ts = TangentSpace()\n",
    "tsfeat_train = ts.fit_transform(covar_train)\n",
    "tsfeat_valid = ts.transform(covar_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(feats, overlap):\n",
    "    if len(feats.shape) == 2:\n",
    "        new_features = np.zeros((feats.shape[0], feats.shape[1] + 2 * overlap))\n",
    "    elif len(feats.shape) == 3:\n",
    "        new_features = np.zeros((feats.shape[0], feats.shape[1] + 2 * overlap, feats.shape[2]))\n",
    "\n",
    "    for i in range(0, feats.shape[0]):\n",
    "        if i > 0:\n",
    "            new_features[i, 0: overlap] = feats[i - 1,-overlap:]\n",
    "        new_features[i, overlap: overlap + feats.shape[1]] = feats[i]\n",
    "        if i < feats.shape[0] - 1:\n",
    "            new_features[i, overlap + feats.shape[1]:] = feats[i + 1, :overlap]\n",
    "    return new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = compute_features(X_train)\n",
    "features_valid = compute_features(X_valid)\n",
    "\n",
    "new_features_train = concatenate(features_train, 40)\n",
    "new_features_valid = concatenate(features_valid, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "# create model instance\n",
    "xgb_reg = XGBRegressor(n_estimators=500, max_depth=5, eta=0.01, subsample=0.7, colsample_bytree=0.8)\n",
    "# fit model\n",
    "xgb_reg.fit(new_features_train, y_train)\n",
    "# make predictions\n",
    "prediction_XGB = xgb_reg.predict(new_features_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1050, 280, 62)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate(X_train, 40).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerFeatureDataset(Dataset):\n",
    "    def __init__(self, ecog, dg, window=2000):\n",
    "        self.ecog = np.float32(ecog.reshape(ecog.shape[0], 1, -1))\n",
    "        self.ecog = (self.ecog - self.ecog.mean()) / self.ecog.std()\n",
    "        self.dg = np.float32(dg)\n",
    "        \n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecog)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.ecog[idx], self.dg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FingerECOGDataset(Dataset):\n",
    "    def __init__(self, ecog, dg, window=200, overlap=100):\n",
    "        self.ecog = np.float32(ecog.reshape(-1, 1, window,ecog.shape[1]))\n",
    "        self.ecog = (self.ecog - self.ecog.mean()) / self.ecog.std()\n",
    "        self.ecog = np.float32(self._concatenate(self.ecog, overlap))\n",
    "        self.dg = np.float32(dg)\n",
    "        \n",
    "    def _concatenate(self, feats, overlap):\n",
    "        new_features = np.zeros((feats.shape[0], feats.shape[1], feats.shape[2] + 2 * overlap, feats.shape[3]))\n",
    "        for i in range(0, feats.shape[0]):\n",
    "            if i > 0:\n",
    "                new_features[i, 0, 0: overlap] = feats[i - 1, 0,-overlap:]\n",
    "            new_features[i, 0, overlap: overlap + feats.shape[2]] = feats[i, 0]\n",
    "            if i < feats.shape[0] - 1:\n",
    "                new_features[i, 0, overlap + feats.shape[2]:] = feats[i + 1, 0, :overlap]\n",
    "        return new_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ecog)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.ecog[idx], self.dg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s1_train = FingerECOGDataset(ecog_1_train.copy(), data_glove_1_train.copy())\n",
    "dataset_s1_valid = FingerECOGDataset(ecog_1_test.copy(), data_glove_1_test.copy())\n",
    "\n",
    "train_loader = DataLoader(dataset_s1_train, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(dataset_s1_valid, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class FingerFlexionCNN(nn.Module):\n",
    "    def __init__(self, num_fingers) -> None:\n",
    "        super(FingerFlexionCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 16, 5)\n",
    "        self.pooling = nn.MaxPool1d(4, stride=2)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.dropout1 = nn.Dropout(0.15)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(16, 32, 5)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(32, 64, 5)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(19840, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_fingers)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1,62), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        #self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (8, 8))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 144 timepoints. \n",
    "        #4, 2, 487\n",
    "        self.fc1 = nn.Linear(104, 64) #4,1,10\n",
    "        self.fc2 = nn.Linear(64, 4) #4,1,10\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "      #  x = x.double()\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "        # print (x.size())\n",
    "        # FC Layer\n",
    "        x = torch.flatten(x, start_dim=1)# 4*2*9) # for T=128\n",
    "        #x = x.view(-1,4 *2 * 9)# 4*2*9) # for T=128\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.module import _addindent\n",
    "\n",
    "\n",
    "class Conv2dWithConstraint(nn.Conv2d):\n",
    "    def __init__(self, *args, max_norm=1, **kwargs):\n",
    "        self.max_norm = max_norm\n",
    "        super(Conv2dWithConstraint, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "        )\n",
    "        return super(Conv2dWithConstraint, self).forward(x)\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def InitialBlocks(self, dropoutRate, *args, **kwargs):\n",
    "        block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.F1, (1, self.kernelLength), stride=1, padding=(0, self.kernelLength // 2), bias=False),\n",
    "            nn.BatchNorm2d(self.F1, momentum=0.01, affine=True, eps=1e-3),\n",
    "\n",
    "            # DepthwiseConv2D =======================\n",
    "            Conv2dWithConstraint(self.F1, self.F1 * self.D, (self.channels, 1), max_norm=1, stride=1, padding=(0, 0),\n",
    "                                 groups=self.F1, bias=False),\n",
    "            # ========================================\n",
    "\n",
    "            nn.BatchNorm2d(self.F1 * self.D, momentum=0.01, affine=True, eps=1e-3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 4), stride=4),\n",
    "            nn.Dropout(p=dropoutRate))\n",
    "        block2 = nn.Sequential(\n",
    "            # SeparableConv2D =======================\n",
    "            nn.Conv2d(self.F1 * self.D, self.F1 * self.D, (1, self.kernelLength2), stride=1,\n",
    "                      padding=(0, self.kernelLength2 // 2), bias=False, groups=self.F1 * self.D),\n",
    "            nn.Conv2d(self.F1 * self.D, self.F2, 1, padding=(0, 0), groups=1, bias=False, stride=1),\n",
    "            # ========================================\n",
    "\n",
    "            nn.BatchNorm2d(self.F2, momentum=0.01, affine=True, eps=1e-3),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d((1, 8), stride=8),\n",
    "            nn.Dropout(p=dropoutRate))\n",
    "        return nn.Sequential(block1, block2)\n",
    "\n",
    "\n",
    "    def ClassifierBlock(self, inputSize, n_classes):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(352, n_classes, bias=False),\n",
    "            nn.Softmax(dim=1))\n",
    "\n",
    "    def CalculateOutSize(self, model, channels, samples):\n",
    "        '''\n",
    "        Calculate the output based on input size.\n",
    "        model is from nn.Module and inputSize is a array.\n",
    "        '''\n",
    "        data = torch.rand(1, 1, channels, samples)\n",
    "        model.eval()\n",
    "        out = model(data).shape\n",
    "        return out[2:]\n",
    "\n",
    "    def __init__(self, n_classes=4, channels=60, samples=151,\n",
    "                 dropoutRate=0.5, kernelLength=64, kernelLength2=16, F1=8,\n",
    "                 D=2, F2=16):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.F1 = F1\n",
    "        self.F2 = F2\n",
    "        self.D = D\n",
    "        self.samples = samples\n",
    "        self.n_classes = n_classes\n",
    "        self.channels = channels\n",
    "        self.kernelLength = kernelLength\n",
    "        self.kernelLength2 = kernelLength2\n",
    "        self.dropoutRate = dropoutRate\n",
    "\n",
    "        self.blocks = self.InitialBlocks(dropoutRate)\n",
    "        self.blockOutputSize = self.CalculateOutSize(self.blocks, channels, samples)\n",
    "        self.classifierBlock = self.ClassifierBlock(self.F2 * self.blockOutputSize[1], n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        x = x.view(x.size()[0], -1)  # Flatten\n",
    "        x = self.classifierBlock(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def categorical_cross_entropy(y_pred, y_true):\n",
    "    # y_pred = y_pred.cuda()\n",
    "    # y_true = y_true.cuda()\n",
    "    y_pred = torch.clamp(y_pred, 1e-9, 1 - 1e-9)\n",
    "    return -(y_true * torch.log(y_pred)).sum(dim=1).mean()\n",
    "\n",
    "def torch_summarize(model, show_weights=True, show_parameters=True):\n",
    "    \"\"\"Summarizes torch model by showing trainable parameters and weights.\"\"\"\n",
    "    tmpstr = model.__class__.__name__ + ' (\\n'\n",
    "    for key, module in model._modules.items():\n",
    "        # if it contains layers let call it recursively to get params and weights\n",
    "        if type(module) in [\n",
    "            torch.nn.modules.container.Container,\n",
    "            torch.nn.modules.container.Sequential\n",
    "        ]:\n",
    "            modstr = torch_summarize(module)\n",
    "        else:\n",
    "            modstr = module.__repr__()\n",
    "        modstr = _addindent(modstr, 2)\n",
    "\n",
    "        params = sum([np.prod(p.size()) for p in module.parameters()])\n",
    "        weights = tuple([tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "        tmpstr += '  (' + key + '): ' + modstr\n",
    "        if show_weights:\n",
    "            tmpstr += ', weights={}'.format(weights)\n",
    "        if show_parameters:\n",
    "            tmpstr +=  ', parameters={}'.format(params)\n",
    "        tmpstr += '\\n'\n",
    "\n",
    "    tmpstr = tmpstr + ')'\n",
    "    return tmpstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train loss: 0.586 | Train Cor: -0.036 | Valid loss: 0.212 | Valid Cor: -0.07221624936576038\n",
      "Epoch 2 | Train loss: 0.565 | Train Cor: -0.014 | Valid loss: 0.161 | Valid Cor: -0.05836394604397254\n",
      "Epoch 3 | Train loss: 0.501 | Train Cor: -0.054 | Valid loss: 0.165 | Valid Cor: -0.054167661383336216\n",
      "Epoch 4 | Train loss: 0.503 | Train Cor: -0.051 | Valid loss: 0.166 | Valid Cor: -0.057242378916905944\n",
      "Epoch 5 | Train loss: 0.500 | Train Cor: 0.004 | Valid loss: 0.163 | Valid Cor: -0.04840970935414928\n",
      "Epoch 6 | Train loss: 0.495 | Train Cor: -0.022 | Valid loss: 0.161 | Valid Cor: -0.11725007128235547\n",
      "Epoch 7 | Train loss: 0.491 | Train Cor: -0.054 | Valid loss: 0.160 | Valid Cor: -0.06002025669348629\n",
      "Epoch 8 | Train loss: 0.490 | Train Cor: -0.068 | Valid loss: 0.160 | Valid Cor: -0.035377899844206015\n",
      "Epoch 9 | Train loss: 0.485 | Train Cor: -0.031 | Valid loss: 0.159 | Valid Cor: -0.08681183554055165\n",
      "Epoch 10 | Train loss: 0.482 | Train Cor: -0.042 | Valid loss: 0.158 | Valid Cor: -0.1004573809902585\n",
      "Epoch 11 | Train loss: 0.490 | Train Cor: -0.074 | Valid loss: 0.159 | Valid Cor: -0.14646422884954124\n",
      "Epoch 12 | Train loss: 0.475 | Train Cor: -0.071 | Valid loss: 0.160 | Valid Cor: -0.10817021699911351\n",
      "Epoch 13 | Train loss: 0.472 | Train Cor: -0.066 | Valid loss: 0.160 | Valid Cor: -0.11384526364307818\n",
      "Epoch 14 | Train loss: 0.467 | Train Cor: -0.080 | Valid loss: 0.162 | Valid Cor: -0.11408500745655595\n",
      "Epoch 15 | Train loss: 0.462 | Train Cor: -0.092 | Valid loss: 0.161 | Valid Cor: -0.13594541619099215\n",
      "Epoch 16 | Train loss: 0.457 | Train Cor: -0.066 | Valid loss: 0.150 | Valid Cor: -0.0594579566146471\n",
      "Epoch 17 | Train loss: 0.475 | Train Cor: -0.028 | Valid loss: 0.161 | Valid Cor: -0.08483859815239088\n",
      "Epoch 18 | Train loss: 0.448 | Train Cor: -0.056 | Valid loss: 0.161 | Valid Cor: -0.09645474541905333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# print (loss)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = EEGNet() #.cuda(0)\n",
    "#print (net.forward(Variable(torch.Tensor(np.random.rand(1, 1, 120, 64)))))#.cuda(0))))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "net.train()\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pred = []\n",
    "    for (i, (ecog, dg)) in enumerate(train_loader):\n",
    "        # print(ecog.shape)\n",
    "        output = net(ecog)\n",
    "        pred += [output.detach().numpy()]\n",
    "        loss = criterion(output, dg)\n",
    "        # print (loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    pred = np.concatenate(pred)\n",
    "    train_cor = pearsonr(np.repeat(pred, 200, axis=0)[:,0], data_glove_1_train[:,0]).statistic\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    # train_acc = correct / total\n",
    "\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred = []\n",
    "        for (i, (ecog, dg)) in enumerate(test_loader):\n",
    "            output = net(ecog)\n",
    "            pred += [output.detach().numpy()]\n",
    "            loss = criterion(output, dg)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        pred = np.concatenate(pred)\n",
    "        val_cor = pearsonr(np.repeat(pred, 200, axis=0)[:,0], data_glove_1_test[:,0]).statistic\n",
    "        valid_loss = running_loss / len(test_loader)\n",
    "    # print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Acc: {train_acc:.3f} | Valid loss: {valid_loss:.3f} | Valid Acc: {val_cor}')\n",
    "    print(f'Epoch {epoch + 1} | Train loss: {train_loss:.3f} | Train Cor: {train_cor:.3f} | Valid loss: {valid_loss:.3f} | Valid Cor: {val_cor}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False,  True, False])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(output.data, 1)[1] == dg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 4)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 4)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(pred, 500, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0491939338496332"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(np.repeat(pred, 500, axis=0)[:,0], data_glove_1_test[:,0]).statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
